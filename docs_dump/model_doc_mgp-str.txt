
MGP-STR
Overview
The MGP-STR model was proposed in Multi-Granularity Prediction for Scene Text Recognition by Peng Wang, Cheng Da, and Cong Yao. MGP-STR is a conceptually simple yet powerful vision Scene Text Recognition (STR) model, which is built upon the Vision Transformer (ViT). To integrate linguistic knowledge, Multi-Granularity Prediction (MGP) strategy is proposed to inject information from the language modality into the model in an implicit way.
The abstract from the paper is the following:
Scene text recognition (STR) has been an active research topic in computer vision for years. To tackle this challenging problem, numerous innovative methods have been successively proposed and incorporating linguistic knowledge into STR models has recently become a prominent trend. In this work, we first draw inspiration from the recent progress in Vision Transformer (ViT) to construct a conceptually simple yet powerful vision STR model, which is built upon ViT and outperforms previous state-of-the-art models for scene text recognition, including both pure vision models and language-augmented methods. To integrate linguistic knowledge, we further propose a Multi-Granularity Prediction strategy to inject information from the language modality into the model in an implicit way, i.e. , subword representations (BPE and WordPiece) widely-used in NLP are introduced into the output space, in addition to the conventional character level representation, while no independent language model (LM) is adopted. The resultant algorithm (termed MGP-STR) is able to push the performance envelop of STR to an even higher level. Specifically, it achieves an average recognition accuracy of 93.35% on standard benchmarks.

 MGP-STR architecture. Taken from the original paper. 
MGP-STR is trained on two synthetic datasets MJSynth (MJ) and SynthText(http://www.robots.ox.ac.uk/~vgg/data/scenetext/) (ST) without fine-tuning on other datasets. It achieves state-of-the-art results on six standard Latin scene text benchmarks, including 3 regular text datasets (IC13, SVT, IIIT) and 3 irregular ones (IC15, SVTP, CUTE).
This model was contributed by yuekun. The original code can be found here.
Inference example
[MgpstrModel] accepts images as input and generates three types of predictions, which represent textual information at different granularities.
The three types of predictions are fused to give the final prediction result.
The [ViTImageProcessor] class is responsible for preprocessing the input image and
[MgpstrTokenizer] decodes the generated character tokens to the target string. The
[MgpstrProcessor] wraps [ViTImageProcessor] and [MgpstrTokenizer]
into a single instance to both extract the input features and decode the predicted token ids.

Step-by-step Optical Character Recognition (OCR)

from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition
import requests
from PIL import Image
processor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')
model = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')
load image from the IIIT-5k dataset
url = "https://i.postimg.cc/ZKwLg2Gw/367-14.png"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
pixel_values = processor(images=image, return_tensors="pt").pixel_values
outputs = model(pixel_values)
generated_text = processor.batch_decode(outputs.logits)['generated_text']

MgpstrConfig
[[autodoc]] MgpstrConfig
MgpstrTokenizer
[[autodoc]] MgpstrTokenizer
    - save_vocabulary
MgpstrProcessor
[[autodoc]] MgpstrProcessor
    - call
    - batch_decode
MgpstrModel
[[autodoc]] MgpstrModel
    - forward
MgpstrForSceneTextRecognition
[[autodoc]] MgpstrForSceneTextRecognition
    - forward