
SAM
Overview
SAM (Segment Anything Model) was proposed in Segment Anything by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.
The model can be used to predict segmentation masks of any object of interest given an input image. 

The abstract from the paper is the following:
We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.
Tips:

The model predicts binary masks that states the presence or not of the object of interest given an image.
The model predicts much better results if input 2D points and/or input bounding boxes are provided
You can prompt multiple points for the same image, and predict a single mask. 
Fine-tuning the model is not supported yet
According to the paper, textual input should be also supported. However, at this time of writing this seems to be not supported according to the official repository. 

This model was contributed by ybelkada and ArthurZ.
The original code can be found here.
Below is an example on how to run mask generation given an image and a 2D point:
thon
import torch
from PIL import Image
import requests
from transformers import SamModel, SamProcessor
device = "cuda" if torch.cuda.is_available() else "cpu"
model = SamModel.from_pretrained("facebook/sam-vit-huge").to(device)
processor = SamProcessor.from_pretrained("facebook/sam-vit-huge")
img_url = "https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png"
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert("RGB")
input_points = [[[450, 600]]]  # 2D location of a window in the image
inputs = processor(raw_image, input_points=input_points, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model(**inputs)
masks = processor.image_processor.post_process_masks(
    outputs.pred_masks.cpu(), inputs["original_sizes"].cpu(), inputs["reshaped_input_sizes"].cpu()
)
scores = outputs.iou_scores

Resources:

Demo notebook for using the model.
Demo notebook for using the automatic mask generation pipeline.
Demo notebook for inference with MedSAM, a fine-tuned version of SAM on the medical domain.
Demo notebook for fine-tuning the model on custom data.

SamConfig
[[autodoc]] SamConfig
SamVisionConfig
[[autodoc]] SamVisionConfig
SamMaskDecoderConfig
[[autodoc]] SamMaskDecoderConfig
SamPromptEncoderConfig
[[autodoc]] SamPromptEncoderConfig
SamProcessor
[[autodoc]] SamProcessor
SamImageProcessor
[[autodoc]] SamImageProcessor
SamModel
[[autodoc]] SamModel
    - forward
TFSamModel
[[autodoc]] TFSamModel
    - call