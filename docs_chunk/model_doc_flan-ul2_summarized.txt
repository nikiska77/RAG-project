Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the UL2 model released earlier last year. It was fine tuned using the "Flan" prompt tuning and dataset collection. Similar to Flan-T5,  one can directly use FLAN-ul2 weights without finetuning the model. The model is pretty heavy (~40GB in half precision) so if you just want to run the model, make sure you load your model in 8bit, and use device_map="auto" to make sure  you don't have any OOM issue! The original checkpoints can be found here. Google has released the following variants: The original checkpoint can be seen here. The Flan-UL1 checkpoint uses a receptive field of 2048 which makes it more usable for few-shot in-context learning. The original UL2 checkpoint also had mode switch tokens that was rather mandatory to get good performance. In this update/change, we continue training UL2 20B for an additional 100k steps (with small batch) to forget “mode tokens” before applying Flan instruction tuning. This Flan-Ul2 checkpoint does not require mode tokens anymore. It can be downloaded from the Google Play store.