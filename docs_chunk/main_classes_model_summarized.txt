The base classes [PreTrainedModel], [TFPreTrained model], and [FlaxPretrainedModel] implement the common methods for loading/saving a model. The other methods that are common to each model are defined in [~modeling_utils.ModuleUtilsMixin] and [~Modeling_tf_util.TFModuleUtil.Mixin]. In Transformers 4.20.0, the [~ pre-trained-model.from_pretrained] method has been reworked to accommodate large models using Accelerate. This requires Accelerate >= 0.9.0 and PyTorch >= 1.0.0 to work. The full model is first created on the Meta device (with empty weights) and the state dict is then loaded inside it (shard by shard in the case of a sharded checkpoint) This way the maximum RAM used is the full size of the model only. Instead of creating the full model, then loading the pretrained weights inside it, there is an option to create the model as an empty shell, then only materialize its parameters when the Pretrained weights are loaded. This option can be activated with low_cpu_mem_ usage=True. The model is then created with empty weights and theState dict is loaded inside the shell.