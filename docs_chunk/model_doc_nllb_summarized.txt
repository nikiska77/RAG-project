The NLLB model was presented in No Language Left Behind: Scaling Human-Centered Machine Translation by Marta R. Costa-jussà, James Cross, Onur Çelebi,aha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam. The default behaviour for the tokenizer was fixed and thus changed in April 2023. The previous version adds [self.eos_token_id, self.cur_lang_code] at the end of the token sequence for both target and source tokenization. This is wrong as the NllB paper mentions (page 48, 6.1.1 Model Architecture) : "Note that we prefix the source sequence with the source language, as opposed to the targetlanguage" The old behaviour can be done as follows: import NllbTokenizer from transformers and set legacy_behaviour to True. For more details, feel free to check the linked PR and Issue. The new behaviour is described in the next section of this article, and can be downloaded as a download from the GitHub repository here: http://www.nllb.org/blog/2014/04/26/nllB-tokenizer-new-behaviour.