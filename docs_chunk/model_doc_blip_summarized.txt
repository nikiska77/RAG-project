The BLIP model was proposed in BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. BLIP is a model that is able to perform various multi-modal tasks including: Visual Question Answering, Image-Text retrieval (Image-text matching) and Image Captioning. The model was contributed by ybelkada. Code, models, and datasets are released. The original code can be found here. The abstract from the paper is the following:                Vision-Language Pre- training (VLP) has advanced the performance for many vision-language tasks.  However, most existing pre-trained models only excel in either understanding-based tasks or generation- based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision- language understanding and generation tasks. BLip effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision- Language tasks.