NAT is a hierarchical vision transformer based on Neighborhood Attention. Neighborhood Attention is a sliding-window self attention pattern. NAT can run up to 40% faster than Swin's WSA while using up to 25% lessmemory. Experimental results on NAT are competitive;NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4 per cent mIoU on ADE20K. It can be used as a backbone. When output_hidden_states = True, NAT will output both hidden_states and reshaped_hiddenÂ states. The shape of the hidden states is based onbatch size, height, width, num_channels. It is possible to use the AutoImageProcessor API to prepare images for the model. The original code can be found here. The abstract from the paper is the following:*We present Neighborhood Attention (NA), the first efficient and scalable sliding- window attention mechanism for vision. * We further present Neighborhood attention extension (NAT), a new hierarchical transformer design based on NAthat boosts image classification and downstream vision performance. The full paper is available here: Neighborhood Attention Transformer (NAT) (http://www.nato.org/nato/transformer/NAT/NAT.html)