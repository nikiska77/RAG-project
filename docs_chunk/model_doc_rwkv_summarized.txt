 RWKV model was proposed in this repo. It suggests a tweak in the traditional Transformer attention to make it linear. This can be more efficient than a regular Transformer and can deal with sentence of any length (even if the model uses a fixed context length for training).This model was contributed by sgugger. The original code can be found here. The model can be used as recurrent network: passing inputs for timestamp 0 and. timestamp 1 together is the same as passing inputs at. timestamp 0, then inputs at timestamp 1 along with the. state of timestamp 0 (see example below) It can also be used to train a recurrent network. It has a stopping criteria to make sure the model stops generating when '\n\n' is detected. The full code is available on the GitHub repository: rwkv.model.summarize:  RwkV.model = RwkvModel.from_pretrained("sgugger/rwkv-430M-pile")    def init(self, eos_sequence = [187,187], eo_token_id = 537): self.eos_ sequence = eo.sequence.eo.tokenizer = AutoTokenizer("This is an example.", return_tensors="pt")  def init( self, eo-token = eos-token, atol=1e-5, dim=1), output_whole = output.whole, output_Whole,. atol = 1e- 5)