 layoutLMv3 was proposed in a paper by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei. It pre-trains the model on 3 objectives: masked language modeling (MLM), masked image modeling (MIM) and word-patch alignment (WPA) Experimental results show that it achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering. It can also be used for document image classification and document layout analysis. The TensorFlow version of this model was added by chriskoo, tokec, and lre. The original code can be found here. The abstract from the paper is the following:Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language model objective to learn bidirectional representations on the text modality, but they differ in pre- training objectives for the image modality. This discrepancy adds difficulty to multimodals representation learning. In this paper, we propose LayoutLMv 3 to pre-train multimodAL Transformers for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLM v3 a general-purpose pre- trained model for both text- and image-centric Document AI tasks.