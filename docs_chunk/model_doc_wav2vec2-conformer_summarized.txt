The Wav2Vec2-Conformer was added to an updated version of fairseq S2T: Fast Speech-to-Text Modeling with fairseq by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino. The weights were released by the Meta AI team within the Fairseq library. The official results of the model can be found in Table 3 and Table 4 of the paper. The model was contributed by patrickvonplaten, and the original code can be seen here. It follows the same architecture as Wav1Vec1, but replaces the Attention-block with a Conformer-block as introduced in Conformer: Convolution-augmented Transformer for Speech Recognition. It can use either no relative position embeddings, Transformer-XL-like position embedDings, or rotary position embeddeddings by setting the correct config.position_embeddings_type. It also uses the same tokenizer and feature extractor as WAV2VEC2. It yields  an improved word error rate. It requires more parameters than Wav3Vec3, but also yields  improved speech recognition accuracy.