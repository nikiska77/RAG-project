The LiLT model was proposed in LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding by Jiapeng Wang, Lianwen Jin, Kai Ding. LiLT allows to combine any pre-trained RoBERTa text encoder with a lightweight layout transformer. It can be pre- trained on the structured documents of a single language and then fine-tuned on other languages with the corresponding off-the-shelf monolingual/multilingual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure. This model was contributed by nielsr. The original code can be found here. The script will result in config.json and pytorch_model.bin files being stored locally. For more information on how to use the model, see the LiltModel website. The model can be downloaded from the HuggingFace website. It is available in English, French, German, Spanish, Italian, Japanese, and Korean. The code for the model can also be found on the HuggedFace website, and the code for lilt-roberta-en-base is available on the Hackage site.