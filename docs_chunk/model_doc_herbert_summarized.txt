The HerBERT model was proposed in KLEJ: Comprehensive Benchmark for Polish Language Understanding by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, and Ireneusz Gawlik. It is a BERT-based Language Model trained on Polish Corpora using only MLM objective with dynamicmasking of whole words. It has the best average performance and obtains the best results for three out of nine tasks. The original code can be found here. The abstract from the paper is the following: "In recent years, a series of Transformer-based models unlocked major improvements in general natural language Understanding (NLU) tasks. We introduce a comprehensive multi-task benchmark for the Polish language," it adds. "We also introduce a new sentiment analysis task for the e-commerce domain, named Allegro Reviews (AR) To ensure a common evaluation scheme and promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains andapplications. Finally, we provide an extensive evaluation, including several standard baselines and recently proposed, multilingual Trans transformer-basedmodels. The model was contributed by rxmroczowski and rxrgawlik. The full paper is available here.