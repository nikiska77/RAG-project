MobileBERT is a bidirectional transformer based on the BERT model. It is 4.3x smaller and 5.5x faster than BERT_BASE. It can be generically applied to downstream NLP tasks via simple fine-tuning. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUEscore o 77.7 (0.6 lower than Bert_Bases) On the SQuAD v1.1/v2.0 question answering task, Mobilebert achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_LARGE) It is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than the left. This model was contributed by vshampor. The original code can be found here. The paper was written by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny                Zhou. The abstract from the paper is the following:. Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERt for compressing and accelerating the popular BERT models.