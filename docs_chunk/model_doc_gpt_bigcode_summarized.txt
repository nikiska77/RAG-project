The GPTBigCode model was proposed in SantaCoder: don't reach for the stars! by BigCode. The model is an optimized GPT2 model. The abstract from the paper is the following: The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7b) in both left-to.-right generation and infilling on theJava, JavaScript and Python portions of MultiPL E. All models are released under an OpenRAIL license at this https URL. The paper was published by the BigCode team. It is available online at: http://www.bigcode.org/tech/tech-report/gpt-big-code-model-model.html.