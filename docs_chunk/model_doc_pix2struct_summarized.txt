Pix2Struct is a pretrained image-to-text model for purely visual language understanding. It can be finetuned on tasks containing visually-situated language. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It has been fine tuned on a variety of tasks and datasets, ranging from image captioning, visual question answering (VQA) over different inputs (books, charts, science diagrams), captioning UI components etc. The full list can be found in Table 1 of the paper. We advise you to use these models for the tasks they have been fine tuning on. If you want to use Pix2Struct for image captioned, you should use the model fine tuned. on the natural images captioning dataset and so on. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images. We also introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. We recommend that you use the Pix2 structural model for captioning images and UI captioning.