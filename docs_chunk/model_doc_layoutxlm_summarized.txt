 layoutXLM is a multilingual extension of the LayoutLMv2 model trained on 53 languages. It aims tobridge the language barriers for visually-rich document understanding. It has its own tokenizer, based on [LayoutXLMTokenizer]/[ layoutXL MTokenizerFast] You can directly plug in the weights of LayoutXLM into a LayoutLM v2 model. The original code can be found here. The abstract from the paper is the following:Multimodal pre-training with text, layout, and image has achieved SOTA performance for visual understanding tasks. Experiment results show that the LayoutX LM model has significantly outperformed the existing SOTA models on the XFUN dataset. The paper was written by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, ChaZhang, Furu Wei, and Tengrong Lv. The code was contributed by nielsr.r. The full paper is available on the arXiv.org site. It can also be downloaded from the arxiv.com/v1.1/v2.0/layoutxlm-base. It's available in English, Chinese, Japanese, Spanish, French, Italian, German, Portuguese and French.