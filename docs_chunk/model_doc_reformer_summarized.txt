The Reformer model was proposed in the paper Reformer: The Efficient Transformer by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya. It uses Axial Positional Encodings to avoid having a huge positional encoding matrix when the sequence length is very big. It also uses LSH (local-sensitive hashing) attention to avoid computing the full product query-key in the attention layers. The resulting model, the Reformer, performs on par with Transformer models, while being much more memory-efficient and much faster on long sequences. It does not work with torch.nn.DataParallel due to a bug in PyTorch, see issue #36035. The Authors' code can be found here. The model was contributed by patrickvonplaten. It was first implemented in Google's trax library and developed by the authors of this model's paper. The authors of the paper also wrote a book on the subject called Reformer. The book is available on Amazon.com and is available for pre-order now. It is available in English, Spanish, Portuguese, and Russian. It can be downloaded from the Amazon Kindle store for £3.99. It has a release date of September 2014.