The ViTMSN model was proposed in Masked Siamese Networks for Label-Efficient Learning by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas. The paper presents a joint-embedding architecture to match the prototypesof masked patches with that of the unmasked patches. With this setup, their method yields excellent performance in the low-shot and extreme low- shotregimes. The authors have only released pre-trained weights of the backbone (ImageNet-1k pre-training). So, to use that on your own image classification dataset, use the [ViTMSNForImageClassification] class. Follow this notebook for a detailed tutorial on fine-tuning. The original code can be found here. The model was contributed by sayakpaul. It is available for download from the GitHub repository here. For confidential support, call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org for details. In the U.S. call the National Suicide Prevention Line on 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/.