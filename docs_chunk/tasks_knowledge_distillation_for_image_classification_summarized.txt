Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student) To distill knowledge from one model to another, we take a pre-trained teacher model trained on a certain task (image classification for this case) and randomly initialize a student model. Next, we train the student model to minimize the difference between it's outputs and the teacher's outputs, thus making it mimic the behavior. It was first introduced in Distilling the Knowledge in a Neural Network by Hinton et al. In this guide, we will do task-specific knowledge distillation. We will use the beans dataset for this. This guide demonstrates how you can distill a fine-tuned ViT model to a MobileNet (student model) using the Trainer API of Transformers. It will also show how to apply the map() method of dataset to apply a preprocessing to every split of the dataset. We are using the merve/beans-vit-224 model as teacher model. It's an image classification model, based on google/v it-base-patch16-224-in21k fine- Tuned on beans dataset. The student model is a randomly initialized MobileNetV2.