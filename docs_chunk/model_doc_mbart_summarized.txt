The MBart model was presented in Multilingual Denoising Pre-training for Neural Machine Translation by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov MarjanGhazvininejad, Mike Lewis, Luke Zettlemoyer. It is a sequence-to-sequence denoising auto-encoder pretrained on large-scale monolingualcorpora in many languages using the BART objective. A special language id token is added in both the source and target text. The source text format is X [eos, src_lang_code] where X is the source text. As the multilingual model is multilingual it expects the sequences in a different format. The regular [~MBartTokenizer.__call__] will encode source textformat passed as first argument or with the text_-_keyword, and targetText format passed with thetext_label keyword argument. This model was contributed by valhalla. The Authors' code can be found here and the code of the MBart-50 model can be seen here. The model is intended for translation task, but can be used for other tasks as well. It can be downloaded from the following GitHub repository:https://www.jupiter.com/images/mbart-large-en-ro.