The Pegasus model was proposed in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019. Pegasus is pre-trained jointly on two self-supervised objective functions: Masked Language Modeling (MLM) and a novel summarization specific pretraining objective, called Gap Sentence Generation (GSG) All models are transformer encoder-decoders with 16 layers in each component. The implementation is completely inherited from [BartForConditionalGeneration] Some key configuration and replication results can be found in this Issue. Each checkpoint is 2.2 GB on disk and 568M parameters on disk. The adafactor optimizer is recommended for pegasus fine-tuning. The model was contributed by sshleifer. The Authors' code can befound here. The code can also be found here. For more information on how to use Pegasus, see the PegASUS website or see the pegasus.org website. The pegasus-large version of this article can be downloaded for free from the pgasus website. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or click here.