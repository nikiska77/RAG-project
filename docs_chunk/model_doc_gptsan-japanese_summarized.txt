GPTSAN is a Japanese language model using Switch Transformer. It has the same structure as the model introduced as Prefix LM in the T5 paper. The generate() method can be used to generate text using GPTSAN-Japanese model. It works as a shifted Masked Language Model for Prefix Input tokens. Un-prefixed inputs behave like normal generative models. Spout is pre-trained with random inputs, but you can specify a class of text or an arbitrary vector during fine-tuning. This allows you to indicate the tendency of the generated text. The model has a sparse Feed Forward based on Switch-Transformer. You can also add other layers and train them partially. See the original GPTSAn repository for details. The original version of this article was published on November 14, 2004. It is no longer available. For more information on the GPTSan repository, visit http://www.tanreinama.com/GPTSan-japanese-repositories/GPSAN-Japanese.html. The latest version of the GTSAN repository can be downloaded from the following site: http:// www.tensin.org/GTSAN/JAPAN/GpsAN/Tensin-Japans/TTSAN.html/.