Masked-attention Mask Transformer (Mask2Former) is a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic) Its key components include masked attention, which extracts localized features. It outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO) and instance segmentation ($57.7 mIoU on ADE20K) It uses the same preprocessing and postprocessing steps as MaskFormer. Use [Mask2 formerImageProcessor] or [AutoImageProcesser] to prepare images and optional targets for the model. All three tasks can be solved using [ Mask2 formerForUniversalSegmentation] output. It accepts an optional label_ids_to_fuse argument to fuse instances of the target object/s (e.g. sky) The original code can be found here. The model was contributed by Shivalika Singh and Alara Dirik. The original paper was written by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.