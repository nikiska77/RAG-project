DeepSpeed provides a deepspeed launcher that is easier to use than other launchers unless you are in a SLURM environment. The information in this section isn't not specific to the DeepSpeed integration and is applicable to any multi-node program. If you need to run on a specific GPU, which is different from GPU 0, you can't use CUDA_VISIBLE_DEVICES to limit  the visible scope of available GPUs. Instead, you have to use the following syntax:  deepspeed --include localhost:1 examples/pytorch/translation/run_translation.py                  In this example, we tell DeepSpeed to use GPU 1 (second gpu) for deployment with multiple Nodes. For the duration of this section let's assume that you have 2 nodes with 8 gpus each. And you can reach the first node with ssh hostname1 and second node withssh hostname2, and both must be able to reach each other via ssh locally without a password. Of course, you will need to rename these host (node) names to the actual host names you are working with. For more information please see torchrun.launch. Incidentally, this is also the launcher that replaced torch.distributed.launch a few pytorch versions back.