The LED model was proposed in Longformer: The Long-Document Transformer. It is a drop-in replacement for the standard self-attention mechanism. It combines a localwindowed attention with a task motivated global attention. It consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results onWikiHop and TriviaQA. It works very well on long-range sequence-to-sequence tasks where the input_ids largely exceed a length of 1024 tokens. For summarization, it is advised to put global attention only on the first token. For question answering, it's better to use the pad_to_multiple_of argument instead of the global_attention_mask. For more information, see the Longformer-Encoder-Decoder (LED) blog post and the arXiv version of this article on Arsene Wenger's blog. The article was originally published in the open-source software journal, The Next Web, where it can be downloaded for free. For confidential support, call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For support in the U.S., call the Samaritans on 08457 909090 or visit a local Samaritans branch.