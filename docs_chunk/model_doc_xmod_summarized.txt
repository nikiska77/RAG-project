The X-MOD model was proposed in Lifting the Curse of Multilinguality by Pre-training Modular Transformers by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe. For fine-tuning, the language adapters in each transformer layer are frozen. The main models – base and large – have adapters for 81 languages. The input language needs to be specified so that the correct language adapter can be activated. There are two ways to specify the input language:. By explicitly passing the index of the language adapter for each sample. By setting a default language before using the model:. The original code can be found here and the original documentation is found here. For more information on the X- MOD model, visit the XmodModel website or see the official X-mod blog page. It can be downloaded from the site for free by clicking the link below: http://www.xmodmodel.org/xmod-base.html/. For more details on how to use the model, click the link here:  http:// www.x modmodel.com/x-mod.html%. For more info on the how-to-use-the-model-and-how-it-works-for-you- click here: http: www.XmodModel.com/.