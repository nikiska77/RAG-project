Most of the methods in this document are generally applicable to TPU training for any Keras model. There are a few points that are specific to the HuggingFace ecosystem (hug-o-system?) of Transformers and Datasets, and we’ll make sure to flag them up when we get to them. If you don't need long explanations and just want TPU code samples to get started with, check out our TPU example notebook!What is a TPU? They are hardware designed by Google, which are used to greatly speed up the tensor computations within neural networks, much like GPUs. They can be used for both network training and inference. They are generally accessed through Google’s cloud services, but small TPUs can also be accessed directly for free through Google Colab and Kaggle Kernels. Using TPU Nodes can have some quite unexpected behaviour for people who aren’t used to them! In particular, because the TPU is located on a physically different system to the machine you’re running your Python code on, your data cannot be local to your machine. Instead, data must be stored in Google Cloud Storage where your data pipeline can still access it, even when the pipeline is running on the remote TPU node.If you can fit all your data in memory as np.ndarray or tf.Tensor, then you can Fit() on that data even when using Colab or a T PU Node, without needing to upload it to Google Cloud storage.