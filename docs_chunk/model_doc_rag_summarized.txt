The paper is based on the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, VladimirKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela. The abstract from the paper is the following:Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and preciselymanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. We introduce RAG models where the parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages per token, the other can use different passages per tokens. We fine-tune and evaluate our RAGmodels on a wide range of knowledge- intensive NLP task and set the state of the art on three open domain QA tasks.