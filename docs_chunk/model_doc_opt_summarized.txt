OPT is a series of open-sourced large causal language models which perform similar in performance to GPT3. The model was proposed in Open Pre-trained Transformer Language Models by Meta AI. It was contributed by Arthur Zucker, Younes Belkada, and Patrick Von Platen. If you'reinterested in submitting a resource to be included here, please feel free to open a Pull Request and we will review it. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models. We show that OPT-175B is comparable to G PT-3, while requiring only 1/7th the carbon footprint to develop. The original code can be found here and the source code is available on GitHub. It can be downloaded as a.tar.bz2 file or as a zip file. It is available for download from the GitHub repository at: http://www.jupiter.com/project/opt/v2.0/OPT.html. It has been made available as an open-source version of the Hugging Face course by the University of California, San Diego. The course is available in English, Spanish, and French. It's available on the University's website.