The LeViT model was proposed in a paper by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze. It improves the Vision Transformer (ViT) in performance and efficiency by a few architectural differences such as activation maps with decreasing resolutions in Transformers. They also introduce the attention bias, a new way to integrate positional information in vision transformers. For example, at 80% ImageNet top-1 accuracy, Le ViT is 5 times faster than EfficientNet on CPU. The model was contributed by anugunj. The original code can be found here. The abstract from the paper is the following:*We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. It is suitable to most architectures. Overall, LeVi T significantly outperforms existing convnets and visiontransformers with respect to the speed/accuracy tradeoff. For more information, visit the LeVIT website or see the LeVit blog. The LeVT paper can be read here.