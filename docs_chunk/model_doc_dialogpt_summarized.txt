 DialoGPT is a GPT2 Model trained on 147M conversation-like exchanges extracted from Reddit. It's a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than the left. The pre-trained model and training pipeline are publicly released to facilitate research into neural responsegeneration and the development of more intelligent open-domain dialogue systems. It enables the user to create a chat bot in just 10 lines of code as shown on the model card. The original code can be found here. For more information please confer to the original paper. The abstract from the paper is the following: We present a large, tunable neural conversational response generation model, Dialo GPT (dialogue generative pre- trainedtransformer). Trained on 147m conversation- like exchanges extracted. from Reddit comment chains over a period spanning                from 2005 through 2017. We show that conversational systemsthat leverage Dialo.GPT generate more relevant, contentful and context-consistent responses than strong baselinesystems. We first concatenate all dialog turns within a dialogue session into a long text x_1,, x_N (N is thesequence length), ended by the end-of-text token. We then frame the generation task as language modeling.