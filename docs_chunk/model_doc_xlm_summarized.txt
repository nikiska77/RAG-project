The XLM model was proposed in Cross-lingual Language Model Pretraining by Guillaume Lample, Alexis Conneau. It's a transformer pretrained using one of the following objectives: causal language modeling, masked language modeling or Translation Language Modeling. The model has many different checkpoints, which were trained using different objectives: CLM, MLM or TLM. There are three different type of training for this model and the library provides checkpoints for all of them. The code and pretrained models will be made publicly available. Check out the multi-lingually page for more information. The original code can be found here. The library provides three different types of training: causal, masked and translation. The full code and models are available on the library's website. The language model was contributed by thomwolf. For more information about the library, see the website's page on the language model. For the full model, visit the site's pages on the languages and language models page. The languages and the language models were contributed by Thomwolf, Guillaumes Lample and Alexis Conneau. for the language modeling and language model part of the project. The project is open-source and can be downloaded from the GitHub site. For. more information, please visit the project's page.