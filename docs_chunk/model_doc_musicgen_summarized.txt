The MusicGen model was proposed in the paper Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre DÃ©fossez. It is a single stage auto-regressive Transformer model capable of generating high-quality music samples conditioned on text descriptions or audio prompts. The model is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need to cascade multiple models to predict a set of codebooks. After downloading the original checkpoints from here, you can convert them using the conversion script available at/transformers. The original code can be found here. The pre-trained checkpoints can befound on theHugging Face Hub. The conversion script is available at /transformers/transformer/musicgen. It can also be found on the/HuggingFaceHub/code page. It has been translated from the original code by sanchit-gandhi and is available for download at/HackHuggingfaceHub. It will take a few minutes to download and install the pre- trained checkpoints. For more information on how to convert the original codes, visit HackHugging face Hub.