The Vision Transformer (ViT) model was proposed in An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining very good results compared to familiar convolutional architectures. The authors of DeiT also released more efficiently trained ViT models, which you can directly plug into [ViTModel] or [ ViTForImageClassification]. There are 4 variants available (in 3 different sizes): facebook/deit-tiny-patch16-224, facebook/ deit-base-patch 16-224 and facebook/Deit- base-patch15-384. The ViT architecture is based on the Convolutional Network (CNN) architecture. It can be used to replace certain components of CNNs while keeping their overall structure in place. For more information, visit the ViT website.