The MPNet model was proposed in MPNet: Masked and Permuted Pre-training for Language Understanding by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu. The abstract from the paper is the following: BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful models. XLNet does not leverage the full position information of a sentence and suffers from position discrepancy between pre- training and fine-tuning. MPNet leverages the dependency among predicted tokens through permuted language modeling, and takes auxiliary position information as input to make the model see a full sentence. Experimental results show that MPNet outperforms MLM and PLM by a largemargin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g.,BERT, XLNet, RoBERTa) under the same model setting. We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine,tune on a variety of down-streaming tasks (GLUE, SQuAD, etc). Just separate your segments with the separation tokenizer.sep_token (or [sep).