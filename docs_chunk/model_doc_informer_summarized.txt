The Informer model was proposed in Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. This method introduces a Probabilistic Attention mechanism to select the "active" queries rather than the "lazy" queries. It provides a sparse Transformer thus mitigating the quadratic compute and memory requirements of vanilla attention. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started. The original code can be found here. The abstract from the paper is the following: Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. The generative style decoder, while conceptually simple, predicts the long time- series sequences at one forward operation rather than a step-by-step way. The self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences.