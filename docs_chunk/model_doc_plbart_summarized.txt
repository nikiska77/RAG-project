The PLBART model was proposed in Unified Pre-training for Program Understanding and Generation by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang. It can be used to perform code-summarization, code-generation, and code-translation tasks. The model is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding. As the model is multilingual it expects the sequences in a different format. A special language id token is added in both the source and target text. The source text format is X [eos, src_lang_code] where X is the source text. A target text format of [tgt_lang-code] X is never used. This model was contributed by gchhablani. The Authors' code can be found here. For more information on the PLBart model, please visit: http://www.plbart-base.org/PLBart-Model.html. The full paper can be downloaded here: http://www.PLBART-Base.com/Papers/Unified-Pre-training-for-Programming- Understanding-and- Generation.