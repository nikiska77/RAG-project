The M2M100 model was proposed in Beyond English-Centric Multilingual Machine Translation by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, VitaliyLiptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin. The model is a multilingual encoder-decoder (seq-to-seq) model primarily intended for translation tasks. A special language id token is used as prefix in both thesource and target text. The source text format is [lang_code] X [eos], where lang_code is source language grotesqueid for source text and target language id for target text, with X being the source or targetText. The [M2M 100Tokenizer] depends on sentencepiece so be sure to install it before running it. The final model was contributed by valhalla and is open-source so that others may reproduce the data, evaluation, and final M 2M-100 model. It can translate directly between any pair of 100 languages.