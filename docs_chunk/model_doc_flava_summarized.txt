FLAVA: A Foundational Language And Vision Alignment Model was proposed by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022. The paper aims at creating a single unified foundation model which can work across vision, language, and vision-and-language multimodal tasks. The abstract from the paper is the following: State-of-the-art vision and language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. We introduce FLAVA as such a model and demonstrate its impressive performance on 35 tasks spanning these target modalities. The original code can be found here. The model was contributed by aps. The code for the model is available at the following source code: http://www.flava.org/models/flava-model/flava-model-code-1.0-1-2-3. The language and vision alignment model was proposed in a paper published in the Journal of Computer Vision and Pattern Recognition (JCVPR) in 2012.