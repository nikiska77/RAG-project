Chinese-CLIP is an implementation of CLIP (Radford et al., 2021) on a large-scale dataset of Chinese image-text pairs. It is capable of performing cross-modal retrieval and also playing as a vision backbone for vision tasks like zero-shot image classification, open-domain object detection, etc. The original Chinese- CLIP code is released at this link. The code snippet below shows how to compute image & text features and similarities. The Chinese-ClIP model was contributed by OFA-Sys. It can achieve the state-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups of zero- shot learning and finetuning, and it is able to achieve competitive performance in zero-shots image classification based on the evaluation on the ELEVATER benchmark (Li et al, 2022) Our codes, pretrained models, and demos have been released. We develop 5 Chinese CLIP models of multiple sizes, spanning from 77 to 958 million parameters. Furthermore, we propose a two-stage pretraining method, where the model is first trained with the image encoder frozen and then trained with all parameters being optimized, to achieve enhanced model performance. We have released the code snippets below.