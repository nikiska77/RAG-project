ImageGPT is a GPT-2-like model trained to predict the next pixel value. On CIFAR-10, we achieve 96.3% accuracy with a linear probe. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding. The authors pre-trained ImageGPT on smaller input resolutions, such as 32x32 and 64x64. This model was contributed by nielsr, based on this issue. The original code can be found here. The abstract from the paper is the following:.Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels,without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a G PT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. We also achieve 69.0% top-1 accuracy on a linear probing of our features, matching the top supervised pre- trained models. It is almost exactly the same as Gpt-2, with the exception that a different activation function is used.