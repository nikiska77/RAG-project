Autoregressive generation with LLMs is also resource-intensive and should be executed on a GPU for adequate throughput. LLMs often require advanced features like quantization and fine control of the token selection step, which is best done through [~generation.GenerationMixin.generate]. The model_ inputs variable holds the tokenized text input, as well as the attention mask. The tokenizer.from_pretrained call ensures the model is moved to your GPU(s) The load_in_4bit call applies 4-bit dynamic quantization to massively reduce the resource requirements for LLMs. The model is loaded alongside the model in the [~ generation.generationConfig] file. It's a good baseline to begin with an LLM, but this is a good way to begin. We'll talk code in the next part of this article. We hope you'll find it useful. We're looking forward to hearing from you! Back to Mail Online home. Back to the page you came from. The page we came from is: http://www.mistralai.com/blog/2014/01/07/lml-pipeline.html#story-link.php#storylink=cpy. We'd also like to hear from you about your favorite LLM.