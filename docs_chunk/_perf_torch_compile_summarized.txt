 torch.compile() yields up to 30% speed-up during inference. It's useful if you are compiling the model only once instead of every time you infer. max-autotune takes longer than reduce-overhead but results in faster inference. Default mode is fastest for compilation but is not as efficient compared to reduce- overhead for inference time. We benchmarked torch. Compile with different computer vision models, tasks, types of hardware, and batch sizes on torch 2.0.1. We warm up the GPU before inference and take the mean time of 300 inferences, using the same image each time. The benchmarking code for each task can be found at the bottom of the page. For more information about torch.Compile, visit the torch website or visit torch.com/torch-compile-benchmarking-code. The torch website can be accessed at the following URL: http://torch.com/. For more info about torch, visit torch com. For the latest version of torch, go to the torch site or visit Torch http.torch://www.torchedrive.org/. For the most up-to-date information on torch, see the torch site.