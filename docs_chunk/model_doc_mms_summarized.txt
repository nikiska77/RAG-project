The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages. We also built a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Here are the different models open sourced in the MMS project. The models and code are originally released here. We have add them to the transformers framework, making them easier to use. For best accuracy, use the mms-1b-all model. All ASR models accept a float array corresponding to the raw waveforms of the speech signal. The raw waveform should be pre-processed with [Wav2Vec2FeatureExtractor]. The models were trained using connectionist temporal classification (CTC) The ASR model checkpoints can be found here : mm-1B-fl102, mms/fl102 and mm/fl1107. The code for the models is available here.