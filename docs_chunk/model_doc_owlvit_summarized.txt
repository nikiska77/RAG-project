OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text. It uses a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. The original code can be found here. The abstract from the paper is the following:Combining simple architectures with large-scale pre- Training has led to massive improvements in image classification. For object detection, pre- training and scaling approaches are less well established, especially in the long-tailed and open-Vocabulary setting, where training data is relatively scarce. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image- Conditioned object detection. Code and models are available on GitHub. The model was contributed by adirik. It is available for download from the GitHub repository here. For more information on OWL- ViT, visit the Open-World Localization section of the project page or the Open World Localization page of the OpenWorld Localisation project.