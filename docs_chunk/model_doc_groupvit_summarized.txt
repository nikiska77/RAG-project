GroupViT is a vision-language model that can perform zero-shot semantic segmentation on any given vocabulary categories. The model was proposed in GroupViT: Semantic Segmentation Emerges from Text Supervision by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang. It achieves a zero- shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% m me on P ASCAL Context datasets, and performs competitively to state-of-the-art transfer-learning methods requiring greater levels of supervision. There is a list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with Group ViT. The quickest way to get started is by checking the example notebooks (example notebooks can be found here) and checking the forward of the GroupVi TModel to get the segmentation logits of input texts. The TensorFlow version of the model was contributed by ariG23498 with the help of Yih-Dar SHIEH, Amy Roberts, and Joao Gante. The original code can befound here.