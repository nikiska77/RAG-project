BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer in between them. Most notably, it improves upon Flamingo, an 80 billion parameter model, by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. It can be used for conditional text generation given an image and an optional text prompt. At inference time, it's recommended to use the [generate] method. The original code can be found here. The model was contributed by nielsr. It was created by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. It is currently being developed by a group of researchers at the University of California, San Diego. It has been described as a 'generic and efficient pre-training strategy' that bootstraps vision-language pre- training from off-the-shelf image encoder and language model. It also has emerging capabilities ofzero-shot image-to-text generation that can follow natural language instructions. It's been called the 'Blip2Processor' The model has been created by a team of researchers and is being worked on by a number of teams.