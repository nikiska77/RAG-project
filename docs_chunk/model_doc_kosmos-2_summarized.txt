The KOSMOS-2 model was proposed in Kosmos-2: Grounding Multimodal Large Language Models to the World by Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei. The work lays out the foundation for the development of Embodiment AI. This work sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. The data format issimilar to “hyperlinks” that connect the object regions in an image to their text span in the corresponding caption. The spatial coordinates of the bounding boxes in the dataset are converted to a sequence of location tokens, which are appended to their respective text spans. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos.-2 integrates the grounding capability into downstream applications. The code and pretrained models are available at https://aka.ms/kosmos-1.html. The abstract from the paper is the following: We introduce Kosmos.-2, a Multimmodal Large language Model (MLLM), enabling new capabilities of perceiving object descriptions and grounding text.