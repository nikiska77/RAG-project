InstructBLIP leverages the BLIP-2 architecture for visual instruction tuning. It also feeds the text prompt (instruction) to the Q-Former architecture. The resulting InstructBLIP models achieve state-of-the-art zero-shot performance across all 13 held-out datasets. The models also lead to state of theart performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA IMG) This model was contributed by nielsr. The original code can be found here. The abstract from the paper is the following:General-purpose language models that can solve various language-domain tasks have emerged driven by the pre-training and instruction-tuning pipeline. However, building general-purpose vision-language models is challenging due to the increased task discrepancy introduced by the additional visual input. In this paper, we conduct a systematic and comprehensive study on vision- language instruction tuning based on thePre-trained BLip-2 models. We gather a wide variety of 26 publicly available datasets, transform them into instruction tuning format and categorize them into two clusters for held-in instruction tuning and held- out zero- shot evaluation. Additionally, we introduce instruction-aware visual feature extraction, a crucial method that enables the model to extract informative features tailored to the given instruction.