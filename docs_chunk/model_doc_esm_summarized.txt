This page provides code and pre-trained weights for Transformer protein language models from Meta AI. It provides the state-of-the-art ESMFold and ESM-2, and the previously released E SM-1b and Esm-1v. The first version of this paper was preprinted in 2019. It was released with the paper Language models of protein sequences at the scale of evolution. Protein language modeling is a logical step toward predictive and generative artificial intelligence. It is a combination of scale in data and model capacity enabled by unsupervised learning. It has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. It enables accurate structure prediction. It can also be used to predict folded protein structures with state of the art accuracy. It will be used in the next version of the paper, which will be released in February 2019. For more information, visit Meta AI's Fundamental                 AI Research Team, or go to: http://www.metacai.com/fundamental-ai/faq.html#faq-faq_faq, or visit the Meta AI website, which is also open-source, for more information.