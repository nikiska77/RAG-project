Training large transformer models and deploying them to production present various challenges. This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. The guides are divided into training and inference sections, as each comes with different challenges and solutions.Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU for training or CPU vs. GPU for inference. The methods that you can apply to improve training efficiency on a single GPU extend to other setups such as multiple GPU. There are also techniques that are specific to multi- GPU or CPU training, and we cover them in separate sections. In the following sections we go through the steps to run inference on CPU and single/multi-GPU setups. We also cover how to use XLA integration for TensorFlow models to train large models in a production environment. We conclude with an overview of the performance and Scalability of large models and TPUs in the production and testing phase of training and deploying a deep learning rig. The documentation is intended to be a starting point to navigate further to the methods that match your scenario. For more information on how to train and deploy deep learning models in production, see the Tensorflow Training and Deployment Guide.