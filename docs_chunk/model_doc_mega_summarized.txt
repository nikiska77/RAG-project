The MEGA model was proposed in Mega: Moving Average Equipped Gated Attention by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Each encoder layer has a multi-headed exponential moving average in addition to a single head of standard dot-product attention. This allows MEGA to perform competitively to Transformers on standard benchmarks including LRA. MEGA's compute efficiency allows it to scale to very long sequences, making it an attractive option for long-document NLP tasks. If using MEGA as a decoder, be sure to set bidirectional=False to avoid errors with default bid Directional=True. Utilize chunking with MegaConfig.use_chunking and control chunk size withMegaConfig.chunk_size. Use the MEGA paper for examples of architectural specs which perform well in various settings. The original code can be found here. The model was contributed by mnaylor and the code for MEGA can be downloaded from the project's GitHub page. For more information on MEGA, please visit the project website or the official MEGA website. The MEGA project page can be viewed here.