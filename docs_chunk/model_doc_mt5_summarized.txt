mT5 is a massively multilingual pre-trained text-to-text transformer. It was developed by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, and Colin Raffel. All of the code and modelcheckpoints used in this work are publicly available. Google has released the following variants:MT5-small, MT5-large, and MT5Tokenizer. The original code can be found here. The abstract from the paper is the following: The recent "Text- to-Text Transfer Transformer" (T5) leveraged a unified text-To-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. We detail the design and modified training of mT5. We also describe a simple technique to prevent "accidental translation" in the zero-shot setting, where aGenerative model chooses to (partially) translate its prediction into the wrong language. We show its performance on many multilingualbenchmarks. The full paper is available at: http://www.researchers.com/mt5-transformer-and-training.