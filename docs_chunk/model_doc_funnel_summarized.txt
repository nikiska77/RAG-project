The Funnel Transformer model was proposed in the paper Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing. It is a bidirectional transformer model, likeBERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks in computer vision. The model can be used directly for tasks that just require a sentence summary. For other tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same sequence length as the input. The original code can be found here. The abstract from the paper is the following: With comparable or fewer FLOPs, Funnel.Transformer outperforms the standard Transformer on a wide variety of sequence-level prediction tasks, including text. classification, language understanding, and reading. The full model was contributed by sgugger. It can be downloaded from his GitHub page here: http://www.sguggers.com/sugger/funnel-transformer-model-and-predictive-language-parsing-tasks-with-a-full-size-decoder-for-multiple-choice-targets.