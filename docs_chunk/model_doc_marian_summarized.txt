Marian is a framework for translation models, using the same models as BART. Translations should be similar, but not identical to output in the test set linked to in each model card. Each model is about 298 MB on disk, there are more than 1,000 models. Models were originally trained by JÃ¶rg Tiedemann using the Marian C++ library, which supports fast training and translation. All models are transformer encoder-decoders with 6 layers in each component. The language codes used to name models are inconsistent. Two digit codes can usually be found here. Three digit codes require googling "language grotesque  code {code}". The 80 opus models that require BPE preprocessing are not supported. The model starts generating with pad_token_id (which has 0 as a token_embedding) as the prefix (Bart uses <s/>).Code to bulk convert models can be found in convert_marian_to_pytorch.py. The first 1000 models use ISO-639-2 codes to identify languages, the second group use a combination of ISO- 639-5 codes and ISO-839-1 codes. The list of supported language pairs can be seen here. The models were converted in two stages.