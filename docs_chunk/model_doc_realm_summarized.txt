The REALM model was proposed in REALM: Retrieval-Augmented Language Model Pre-Training by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It's a Retrival-augmented language model that firstly retrieves documents from a textual knowledge corpus and thenutilizes retrieved documents to process question answering tasks. The original code can be found here. The abstract from the paper is the following:Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. We show how to pre-train such a knowledge retriever in an unsupervised manner, using masked languagemodeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We compare against state-of-the-art models for both Explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previousmethods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such asinterpretability and modularity. The model was contributed by qqaatw. The full paper can be seen here: Â REALM.