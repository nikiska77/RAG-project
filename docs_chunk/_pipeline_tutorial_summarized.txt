The [pipeline] makes it simple to use any model from the Hub for inference on any language, computer vision, speech, and multimodal tasks. Even if you don't have experience with a specific modality or aren't familiar with the underlying code behind the models, you can still use them for inference. This tutorial will teach you to: use a [p Pipeline] for inference, use a specific tokenizer or model, and use a model for audio, vision, and multi-million dollar tasks. The [pippedeline] abstraction contains the task-specific pipelines for each task. The general abstraction also contains the general abstraction which contains all task- specific pipelines. The example below is for automatic speech recognition (ASR), orspeech-to-text. The Whisper large-v2 model from OpenAI was released 2 years later than Wav2Vec2, and was trained on close to 10x more data. It also has the added benefit of predicting punctuation and casing, neither of which are possible with Wav 2Vec 2. The full tutorial can be downloaded from the GitHub repository here: http://www.huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac.