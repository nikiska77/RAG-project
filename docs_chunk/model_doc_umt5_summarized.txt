The UMT5 model was proposed in UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant. The original code can be found here. Google has released the following variants: Google/umt5-smallGoogle/um t5-xlGoogle/ummot5-xxl. The conversion script is also different because the model was saved in t5x's latest checkpoint. The model was only pre-trained on mC4 excluding any supervised training. It has a non-shared relative positional bias that is computed for each layer. This means that the model set has_relative_bias for each layers. If you are doing multi-task fine-tuning, you should use a prefix. For more information on how to use the model, visit the UMT 5 website. The full paper is available at: http://www.t5x.com/unimax/uMT5/uMox.html. It is available in English, Spanish, French, Portuguese, and Arabic. It can also be downloaded from the website:http://t5xs.com/.