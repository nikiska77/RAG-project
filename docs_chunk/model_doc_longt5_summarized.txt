The LongT5 model is an extension of the T5 model. It enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2) Transient-Global attention. It is capable of handling input sequences of a length up to 16,384 tokens. It uses a different pre-training objective inspired by the pre- training of [PegasusForConditionalGeneration]. It is designed to work efficiently and very well on long-range sequence-to-sequence tasks. It can outperform the original T5 models on question answering tasks. This model was contributed by stancld. The original code can be found here. The abstract from the paper is the following: "LongT5: Efficient Text-To-Text Transformer for Long Sequences" It was written by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's anencoder-decoder transformer pre-trained in a text- to-text denoising generative setting. For more information, see the paper's abstract and the source code. The full paper can be downloaded from: http://www.researchers.com/longt5/paper.html.