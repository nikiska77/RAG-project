Perceiver IO is a generalization of Perceiver to handle arbitrary outputs in addition to arbitrary inputs. The original Perceiver only produced a single classification label. In addition to language, optical flow, and multimodal videos with audio, Perceiver IO can produce (for example) language, audio, and optical flow. The computational complexity of Per receiver IO is linear in the input and output size and the bulk of the processing occurs in the latent space. This means, for example, that the model can do BERT-style masked language modeling directly using bytes instead of tokenized inputs. It achieves state-of-the-artperformance on Sintel. The full PerceiverIO model achieves strong results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, multi-task and multi-modal domains. It matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization. It scales linearly with data size, but now with respect to both input andoutput sizes. It has been described as a "general architecture for Structured Inputs &                Outputs" by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, David Ding, Skanda Koppula, Daniel Zoran.