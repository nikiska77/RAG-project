The ErnieM model was proposed in ERNIE-M: Enhanced Multilingual Representation by AligningCross-lingual Semantics with Monolingual Corpora by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang. Instead of using MaskedLM for pretraining (like BERT) the authors used two novel techniques: Cross-attention Masked Language Modeling and Back-translation Masked language Modeling. For now these two LMHead objectives are not implemented here. It is a BERT-like model so it is a stacked Transformer Encoder. It delivers new state-of-the-art results in various cross-lingually downstream tasks. The original code can be found here. The model was contributed by Susnato Dhar. The abstract from the paper is the following:Recent studies have demonstrated that pre-trained cross-ledual models achieve impressive performance in downstream cross- lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. We generate pseudo-parallel sentence pairs on amonolingual corpus to enable the learning of semantic alignments between different languages, thereby enhancing the semantic modeling of cross-lingsual models.