The OpenAI GPT model was proposed in Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever. It's a causal (unidirectional) transformer that is pre-trained using language modeling on a large corpus will long range dependencies, the Toronto Book Corpus. The model is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than the left. The original code can be found here. If you want to reproduce the original tokenization process, you will need to install ftfy and SpaCy. For more information on how to use Write With Transformer, please visit Hugging Face's website or the OpenAI site. The OpenAI page for GPT can be seen at: http://www.openai.org/gpt/GPT-2.html. It can be downloaded from the site for free by clicking the link below: http:/www. openai.com/gPT/gpspip/GPSpip.php. It will install spacy ftfyand SpaCy on your PC or Mac. The code for the GPT page is available on the site here.