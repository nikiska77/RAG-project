The XLM-RoBERTa-XL model was proposed in Larger-Scale Transformers for Multilingual Masked Language Modeling by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau. It is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does not require lang tensors to understand which language is used. It should be able to determine the correct language from the input ids. The model was contributed by Soonhwan-Kwon and stefan-it. The original code can be found here. The code and models were made publicly available by the authors of the paper. The abstract from the paper is the following:Recent work has demonstrated the effectiveness of cross-lingual language model pretraining. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low- resource languages. We make our models publicly available. The full paper can be downloaded from the following site: http://www.xlm-project.org/xlms/xlsx.html.