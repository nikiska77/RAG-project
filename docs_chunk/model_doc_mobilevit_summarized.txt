MobileViT is a light-weight and general-purpose vision transformer for mobile devices. It replaces local processing in convolutions with global processing using transformers. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters. It is 5.7% more accurate than MobileNetv3 for a similar number of parameters on the MS-COCO object detection task. It does not work on sequence data but on batches of images. Unlike ViT, there are no embeddings. The backbone model outputs a feature map. You can follow this tutorial for a lightweight introduction. The original code and weights can be found here. The TensorFlow version of the model was contributed by sayakpaul. It can be used to prepare images for the model. Note that if you do your own preprocessing, the model can be preprocessed using [MobileVi TImageProcessor] to prepare the model for you. The model is more like a CNN than a Transformer model. For more information, visit the Mobile ViT website. It was written by Sachin Mehta and Mohammad Rastegari and published in the journal Mobile Vision Transformer: Light-weight, General-purpose, and Mobile-friendly Vision Trans transformer.