The BigBird model was proposed in Big Bird: Transformers for Longer Sequences. BigBird is a sparse-attention transformer which extends Transformer based models, such as BERT to much longer sequences. Theoretically, it has been shown that applying sparse, global, and random attention approximates full attention. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. The original code can be found here. For an in-detail explanation on how BigBird's attention works, see this blog post. The proposed sparse attention can handle sequences of length up to 20x that of BERT or RoBERTa. We also propose novel applications to genomics data. For more information on BigBird, see the BigBird blog post and the Big Bird project page. The BigBird project page can be seen here and the project page is available on the Google Play store. The project page has been updated to include the latest version of BigBird. The full version of this article can be downloaded from the GooglePlay site. The latest version also includes the latest versions of the BERT and RoberTa implementations. The BERT implementation can be viewed here. The RoBER Ta implementation is available from the GitHub site.