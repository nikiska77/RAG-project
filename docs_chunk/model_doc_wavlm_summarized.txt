The WavLM model was proposed in a paper by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian. The abstract from the paper is the following:Self-supervised learning (SSL) achieves great success in speech recognition. As speech signal contains multi-faceted information including speaker.identity, paralinguistics, spoken content, etc., learning universal representations for all speech. tasks is difficult. In this paper, we propose a new pre-trained model, Wav LM, to solve full-stack downstream speech tasks. We first equip the Transformer structure with gated relative position bias to improve its capability onrecognition tasks. For better speaker discrimination,we propose an utterance mixing training strategy, where overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale upthe training dataset from 60k hours to 94k hours. WavVM Large achieves state-of-the-art performance on the SUPERBbenchmark, and brings significant improvements for various speech processing tasks.