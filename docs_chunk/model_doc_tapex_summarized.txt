This model is in maintenance mode only, we don't accept any new PRs changing its code. If you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0. The model was proposed in TAPEX: Table Pre-training via Learning a Neural SQL Executor by Qian Liu, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. It pre-trains a BART model to solve synthetic SQL queries, after which it can be fine-tuned to answer natural language questions related to tabular data, as well as performing table fact checking. It outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. The abstract from the paper is the following: "TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQLexecutor on the diverse, large-scale and high-quality synthetic corpus" The model has been fine- Tuned on several datasets: SQA (Sequential Question Answering by Microsoft) WTQ (Wiki Table Questions by Stanford University) WikiSQL (by Salesforce) TabFact (by USCB NLP Lab).