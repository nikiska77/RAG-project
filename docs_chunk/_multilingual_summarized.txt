There are several multilingual models in Transformers, and their inference usage differs from monolingual models. Some models, like bert-base-multilingual-uncased, can be used just like amonolingual model. This guide will show you how to use mult bilingual models whose usage differs for inference. The following XLM models use language embeddings to specify the language used at inference: English-French, German, Romanian, XNLI languages, and French-German. The values in these tensors depend on thelanguage used and are identified by the tokenizer's lang2id and id2lang attributes. The language-embeddings are represented as a tensor of the same shape as the input_ids passed to the model. For example, load the xlm-clm-enfr-1024 checkpoint (Causal language modeling,Â English-French) with the following tokenizer: XLMWithLMHeadModel.from_pretrained("xlm/clm/enfr/1024"), with the language-embedding attribute: Xlm:Clm:Enfr/Enro/ENro, with theLanguageEmbedding attributes: XL:Enro, English-Romanian, French.