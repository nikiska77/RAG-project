SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V and FFN layers. It's a bidirectional transformer similar to the BERT model. It runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test set. The SqueezeberT code will be released. The abstract from the paper is the following:Humans read and write hundreds of billions of messages every day. Today's highly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with BERT taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. We demonstrate how to replace several operations in self-attention layers with grouped convolution, and we use this technique in a novel network architecture called SqueezBERT. The model is efficient at predicting masked tokens and at NLU in general, but is not optimal for deploying NLP models at scale. It is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than the left. It relies on the masked language modeling (MLM) objective and is therefore inefficient at predicting masks.