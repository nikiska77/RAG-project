Most transformer models use full attention in the sense that the attention matrix is square. Longformer and reformer are models that try to be more efficient and use a sparse version of the attention Matrix to speed up training. The attention mask is modified to mask the current token (except at the first position), because it will give a query and a key equal. Some preselected input tokens are also given global attention. This is shown in Figure 2d of the paper, see below for a sample attention mask:Using those attention matrices with less parameters then allows the model to have inputs having a bigger sequence length. If you have very long texts, this matrix can be huge and take way too much space on the GPU. To alleviate that, axial positional encodings consist of factorizing that sequence length into a single number, such as 1/2. For example, 1/3 of a sentence could be 1,000, or 1/4 of a paragraph, or even 1/5 of a whole sentence, depending on the content of the text. The model can then use this factorization to build a representation of the whole sentence. It can also be used to build representations of multiple sentences at a time, for example, to represent multiple words at once.