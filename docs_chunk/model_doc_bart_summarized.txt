The Bart model was proposed in BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer on 29 Oct, 2019. It uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a left- to-right decoder ( like GPT) The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains  of up to 6 ROUGE. The model was contributed by sshleifer. The authors' code can be found here. It's usually advised to pad the inputs on the right rather than the left. Use [BartTokenizer] or [~BartModel] to get the proper splitting.