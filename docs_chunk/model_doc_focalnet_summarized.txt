The FocalNet model was proposed in Focal Modulation Networks by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. The authors claim that FocalNets outperform self-attention based models with similar computational costs on the tasks of image classification, object detection, and segmentation. FocalModulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its content, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. The abstract from the paper is the following:We propose focal modulation networks (FocalNet in short), where self-Attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. When transferred to downstream tasks, Focalnets exhibit clear superiority. For object detection with Mask R-CNN, F focalNet base trained with 1\times outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3\times schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2,4, and beats Swin at multi-scale (50.5 v. s. 49.7). Using large Focal net and Mask2former, we achieve 58.5 mIoU for ADE20K semantic segmentating, and 57.5mIo U for ADe20K.