The GIT model was proposed in GIT: A Generative Image-to-text Transformer for Vision and Language. GIT is a decoder-only Transformer that leverages CLIP's vision encoder to condition the model on vision inputs besides text. The model obtains state-of-the-art results on image captioning and visual question answering benchmarks. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks. A list of official Hugging Face and community resources to help you get started with GIT can be found here. If you're interested in submitting a resource to be included here, please submit it here. The original code can be seen here. This model was contributed by nielsr. It is implemented in a very similar way to GPT-2, the only difference being that the model is also conditioned on pixel_values. For more information on how to use GIT, please visit the GIT website or see the Hugging face blog post for more information about how to get started using GIT.