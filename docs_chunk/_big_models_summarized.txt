When you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. In this guide, we explore the solutions Transformers offer to deal with this issue. Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do model.save_pretrained, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the max_shard_size parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model. Note that this is an area of active development, so the APIs explained here may change slightly in the future.summarize:                 Instantiating a big model with a big weight can take up a lot of RAM. If you are using torch.distributed to launch a distributed training, each process will load the pret trained model and store these two copies in RAM. The usual workflow is:Create your model with random weights.Load your pretrained weights.Put those pret trained weights in your random model.