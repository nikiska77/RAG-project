The CANINE model was proposed in CANINE: Pre-training an Efficient Tokenization-Free Encoder for LanguageRepresentation by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting. It's among the first papers that trains a Transformer without using an explicit tokenization step. Instead, the model is trained directly at a Unicode character-level. CANINE outperforms a comparable mBERT model by 28% on TyDi QA, a challenging multilingual benchmark, despite having 28% fewer model parameters. It uses no less than 3 Transformer encoders internally: 2 "shallow" Encoders (which only consist of a single layer) and 1 "deep" encoder (which is a regular BERT encoder) The original code can be found here. The model was contributed by nielsr and can be downloaded from his GitHub page, where you can also find the source code for CANINE by clicking on the link below: CANINE-Pre-training-A-NLP-Model. The code can also be downloaded as a zip file from the GitHub page: CANine-Pre training-A. NLP-model. Zip file: Canine-NPL-Model-Cannine.