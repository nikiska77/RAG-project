The XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does not require lang tensors to understand which language is being used. The model was contributed by stefan-it. The original code can be found here. The paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingually transfer tasks. It will make the model code, data, and models publicly available in the near future. The abstract from the paper is the following:. We show, for the first time, the possibility of multilingual modeling without sacrificingper-language performance;. We also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale.