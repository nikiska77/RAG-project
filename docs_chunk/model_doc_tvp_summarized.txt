The text-visual prompting (TVP) framework was proposed in the paper Text-Visual Prompting for Efficient 2D Temporal Video Grounding by Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding. It aims to predict the starting/ending time points of moments described by a text sentence within a long untrimmed video. The approach employs 2 patterns, known as 'prompts', into both the visual (image-based) and textual (word-based), input components of a TVG model. These prompts provide additional spatial-temporal context, improving the model's ability to accurately determine event timings in the video. Experiments on two benchmark datasets, Charades-STA and ActivityNet Captions, empirically show that the proposed TVP significantly boosts the performance of 2D TVG (e.g., 9.79% improvement onCharades- STA and 30.77% improvement in Activity net Captions) and achieves 5Ã— inference acceleration over TVG using 3D visual features. The proposal was made in a paper published in the journal Computer Vision and Pattern Recognition (CPR) in May 2013. The paper was written by Zhang, Chen, Liu, Jia and Ding.