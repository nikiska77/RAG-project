The T5 model was presented in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to -text format. The pretraining includes both supervised and self-supervised training. Supervised training is conducted on downstream tasks provided by the GLUE and SuperGLUE benchmarks. The model works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task. To facilitate future work on transfer learning for NLP, we release our dataset, pre- trained models, and code. All checkpoints can be found on the hub. The original code was contributed by thomwolf and can be seen here. The dataset and code can befound on thehub and the original code can been found here. For more information, visit the hub and the code page of the T5 paper. The abstract from the paper is the following: