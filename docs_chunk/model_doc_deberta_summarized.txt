The DeBERTa model is based on Google's BERT and Facebook's RoBERTa. It builds on RoberTa with disentangled attention and enhanced mask decoder training with half of the data used in BERT. The code and pre-trained models will be made publicly available at https://github.com/Microsoft/DeBERTa and TF 2.0 implementation was contributed by kamalkraj. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and open a pull request for the model. The original code can be found here. The DeBER Ta code andpre- trained models will also be made public athttps://www.scientistcentral.org/departures/deBERTa-pre-trained-models- TF-2.0- implementation was also contributed bykamalkRAj and contributed by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. The model was proposed in DeberTa: Decoding-enhanced BERT with Disentangled Attention. The abstract from the paper is the following:    Recent progress in pre- trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks.