The Table Transformer model was proposed in PubTables-1M: Towards comprehensive table extraction from unstructured documents byBrandon Smock, Rohith Pesala, Robin Abraham. The authors train 2 DETR models, one for table detection and one fortable structure recognition, dubbed Table Transformers. The original code for the model can be found here. An interesting Github thread with replies from the authors can befound here. It turns out padding of images is quite important for detection. The model was contributed by nielsr. The abstract from the paper is the following:Recently, significant progress has been made applying machine learning to the problem of table structure inference and extraction fromUnstructured Documents. The new dataset, PubT tables-1m, is designed to benchmark progress in table extraction and table structure recognition. It also addresses a significantsource of ground truth inconsistency observed in prior datasets called oversegmentation, using a novel canonicalization procedure. It contains nearly one million tables from scientific articles, and contains detailed header and location information for table structures, making it useful for a wide variety of modeling approaches. We demonstrate that transformer-basedobject detection models produced excellent results for all three tasks of detection, structure recognition and functional analysis without the need for any customized customization for these tasks.