GPT-Sw3 is a collection of large decoder-only pretrained transformer language models. It was developed by AI Sweden in collaboration with RISE and the WASP WARA for Media and Language. It has been trained on a dataset containing320B tokens in Swedish, Norwegian, Danish, Icelandic, English, and programming code. The implementation uses the GPT2Model coupled with our GPTSw3Tokenizer. The model was pretrained using a purposefullycausal language modeling (CLM) objective utilizing the NeMo Megatron GPT implementation. It can be installed with pip install transform and can be used in conjunction with GPT1 and GPT3. It is not available for use in the Swedish language. It does not work in the Danish language or the Norwegian language. For more information on how to use GPT-SW3, please visit the official GPTSw3 website. It's available in English, Swedish, Danish and Norwegian. It also works in the German, Finnish, and Norwegian language as well as the Danish, Norwegian and Icelandic versions of the language. The Swedish version of the model can be downloaded from the Swedish GPT site. It will not be available in the U.S. version of this site. The German version is available from the German GPT website and the Danish version from the Danish GPT page.