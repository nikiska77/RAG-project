The Swin Transformer V2 model was proposed in a paper by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo. It is the largest dense vision model to date, and makes it capable of training with images of up to 1,536Ã—1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. The model is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times more training time. If you're interested in trying out the model, there are a number of resources to help you get started with Swintransformer v2. The original code can be found here. The example script and notebook for the model can be seen here. It was contributed by nandwalritik and is available for download from the GitHub repository. The paper was published in the journal Computer Vision and Pattern Recognition (CSR) and the abstract is available here.