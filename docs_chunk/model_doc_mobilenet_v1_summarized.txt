The MobileNet model was proposed in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam. It is based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. The model predicts 1001 classes: the 1000 classes from ImageNet plus an extra “background” class (index 0) The checkpoint is named mobilenet_v1_depth_size, where 1.0 is the depth multiplier (sometimes also referred to as "alpha" or the width multiplier) and 224 is the resolution of the input images the model was trained on. Even though the checkpoint is trained on images of specific size, the model will work on any size. The smallest supported image size is 32x32. One can use [MobileNetV1ImageProcessor] to prepare images for the model. To use native PyTorch padding behavior, create a [ mobilenetV1Config] with tf_padding = 0. The original code and weights can be found here. The current version of the MobileNet V1 can be downloaded here.