LLaMA is a collection of foundation language models ranging from 7B to 65B parameters. LLaMA-13B outperforms GPT-3 (175B) on most benchmarks. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. We release all our models to the research community. This model was contributed by zphang with contributions from BlackSamorez. The code of the implementation in Hugging Face is based on G PT-NeoX here. The original code can be found here. Weights can be obtained from by filling out this form. After downloading the weights, they will need to be converted to the Hugging face Transformers format using the conversion script. The script can be called with the following (example) command:python src/transformers/models/llama/convert_llama_weights_to_hf.py --model_size 7B --output_dir /output/path /path/to/downloaded/lama/weights. The model and tokenizer can be loaded via:import LlamaForCausalLM, LlamaTokenizer.from_pretrained("/ output/path")