The RemBERT model was proposed in Rethinking Embedding Coupling in Pre-trained Language Models by Hyung Won Chung, Thibault Févry, Henry Tsai, Melvin Johnson, Sebastian Ruder. The abstract from the paper is the following:We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art language models. We show that decoupledembeddings provide increased modeling flexibility, allowing us to significantly improve the efficiency of parameter allocation in the input embedding of multilingual models. Harnessing these findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the number of parameters at the fine-tuning stage. The tokenizer is also similar to the Albert one rather than the BERT one. For more information, see the paper's abstract and the book's resources and instructions on how to use the model in your own language or for other people's to help you train your own models, or for help with training your own languages, visit: http://www.researchgate.com/article/reethinking-embedding-coupling-in-pre-trained-language-models-researchers.