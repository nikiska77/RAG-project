The ALIGN model was proposed in Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig. It can be used for image-text similarity and for zero-shot image classification. ALIGN features a dual-encoder architecture with EfficientNet as its vision encoder and BERT as its text encoder, and learns to align visual and text representations with contrastive learning. Unlike previous work, ALIGN leverages a massive noisy dataset and shows that the scale of the corpus can be use to achieve SOTA representations with a simple recipe. The aligned visual and language representations enables zero- shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image- text retrieval benchmarks. The representations also enable cross-modality search with complex text withcomplex text. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. We show that the Scale of our corpus can make up for its noise and leads to state- of the-art representations even with such a simple learning scheme.