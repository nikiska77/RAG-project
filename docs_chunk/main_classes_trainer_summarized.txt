The [Trainer] class provides an API for feature-complete training in PyTorch. It's used in most of the example scripts. If you're looking to fine-tune a language model like Llama-2 or Mistral on a text dataset, consider using trl's [~trl.SFTTrainer]. It is specially optimized for this particular task and supports sequence packing, LoRA, quantization, and DeepSpeed for efficient scaling to any model size. The API supports distributed training on multiple GPUs/TPUs, mixed precision through [NVIDIA Apex] for NVIDIA GPUs, ROCm APEX for AMD GPUs, and Native AMP for PyTorCh. It is optimized for ðŸ¤— Transformers and is suitable for a broader spectrum of tasks. For more information on how to use the class, see the Trainer documentation. It can also be used to create your own custom training scripts by creating a [TrainingArguments] file. For example, you can use the [Training Arguments] function to access all the points of customization during training. It has a built-in training loop and an evaluation loop. It also has a test and evaluation loop and returns metrics if labels are available on a test set. You can also subclass or override the create_optimizer and create_scheduler methods separately.