The ViTMAE model was proposed in Masked Autoencoders Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick. The paper shows that, by pre-training a Vision Transformer (ViT) to reconstruct pixel values for masked patches, one can get results after fine-tuning that outperform supervised pre- training. The model is based on two core designs: an asymmetric encoder-decoder architecture, and a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. The original code can be found here. The TensorFlow version of the model was contributed by sayakpaul and arieariG23498 (equal contribution). The original model was also contributed by nielsr. The abstract from the paper is the following: "Masked autoen coders (MAE) are scalable self-supervised learners for computer vision. Our scalable approach allows for learning high-capacity                models that generalize well"