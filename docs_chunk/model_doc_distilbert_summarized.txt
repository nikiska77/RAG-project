DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than BERT-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. The model is cheaper to pre-train and we demonstrated its capabilities for on-device computations in a proof-of-concept experiment and a comparative on- device study. It doesn't have options to select the input positions (position_ids input). This could be added if you want to separate your segments with the separation token tokenizer (or [SEP]). The original code for the model can be found here. This model jax version was contributed by victorsanh and kamalkraj, and the full code can be seen here. The full paper is available on the arXiv.com/2013/01/29/distil-bert-paper-1.html. It is available in English, German, French, Spanish, Italian, Portuguese, and Spanish. The paper's abstract can be read here:Â "DistilberT: A Small, Fast, Cheap and Light Transformer Model for BERT"