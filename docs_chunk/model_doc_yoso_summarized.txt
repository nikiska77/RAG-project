The YOSO model was proposed in You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling. In principle, all the Bernoullis random variables can be sampled with a single hash. The algorithm is implemented through custom CUDA kernels, functions written in CUDA C++ that can be executed multiple times on a GPU. The code is available at this https URL. The original code can be found here. The paper was written by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh. The abstract from the paper is the following:Transformer-based models are widely used in natural language processing (NLP) Central to the transformer model is the self-attention mechanism, which captures the interactions of token pairs in the input sequences. Training such models on longer sequences is expensive. In this paper, we show that aBernoulli sampling mechanism based on Locality Sensitive Hashing, decreases the quadratic complexity of such models to linear. We evaluate our algorithm on the GLUE benchmark with standard 512 sequence                 length where we see favorable performance relative to a standard pretrained Transformer. On the Long Range Arena (LRA) benchmark, for evaluating performance on long sequences, our method achieves results consistent with softmax self- attention.