CTRL is a causal (unidirectional) transformer pre-trained using language modeling on a very large corpus of text data. It makes use of control codes to generate text: it requires generations to be started by certain words, sentences or links to generate coherent text. The PyTorch models can take the past_key_values as input, which is the previously computed key/value attention pairs. This provides a potential method for analyzing large amounts of datavia model-based source attribution. It is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than the left. It can be used to generate syntactically coherent text as it can be observed in the run_generation.py example script. Refer to the original implementation for more information on the usage of this argument. The original code can be found here. The model was contributed by keskarnitishr. It was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. See the forward hypertrophic method for more info on how to use this model. It's a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior.