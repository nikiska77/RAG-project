ProphetNet is an encoder-decoder model and can predict n-future tokens for "ngram" language modeling. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. The model architecture is based on the original Transformer, but replaces the “standard” self-attention mechanism in the decoder by a a main self-Attention mechanism and a self and n-stream (predict) self-ATTention mechanism. It is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than the left. It achieves new state-of-the-art results on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. The abstract from the paper is the following:. The ProphetNet model was proposed in ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training, by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei.Zhang, Ming Zhou on 13 Jan, 2020. The Authors' code can be found here.