The BARTpho model was proposed by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen. It is the first public large-scale monolingual sequence-to-sequence models pre-trained for Vietnamese. It uses the "large" architecture and pre-trainingscheme of BART, thus especially suitable for generative NLP tasks. The model was contributed by dqnguyen. The original code can be found here. With TensorFlow 2.0+: import TFAutoModel.from_pretrained("vinai/bartpho-syllable") with torch.no_grad(): features = bartpho(** input_ids) # Models outputs are now tuples. With torch. no_grad()() :features = bart pho(*input_ids), line = "Chúng tôi là những nghiên cứu viên" with tokenizer(line, return_tensors="pt") as line: line = line, tokenizer.no-grad() : features = Bartpho (**input_id), line.last() : feature.last (line), tokenizer (line) : features.last(line)