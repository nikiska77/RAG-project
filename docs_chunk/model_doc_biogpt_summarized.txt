BioGPT follows the Transformer language model backbone, and is pre-trained on 15M PubMed abstracts from scratch. The model can take the past_key_values (for PyTor) to generate syntactically coherent text. It is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than the left. The original code can be found here. The paper was written by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu. The abstract from the paper is the following:Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioG PT on biomedical literature to generate fluent descriptions for biomedical terms.