The Nyströmformer model was proposed in a paper by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. The abstract from the paper is the following:Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences. To address this limitation, we propose a model that exhibits favorable scalability as a function of sequence length. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystrømformer performs favorably relative to other efficient self-Attention methods. Our code is available at this https URL. This model was contributed by novice03. The original code can be found here. For more information on the paper, visit the paper's website here: http://www.nysurvey.org/Nystromformer-Algorithm-Poster-Nyströms-Paper-2.html. The paper is also available at the following URL:http:// www.sensor-survey.com/nysu/Nysu-Survey-Surveillance-Report-2-Niesu.html/.