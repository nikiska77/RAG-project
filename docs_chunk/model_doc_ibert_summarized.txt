I-BERT is a quantized version of RoBERTa. It's a speedup of 2.4 - 4.0x forINT8 inference on a T4 GPU system. The framework has been developed in PyTorch and has been open-sourced. The original code can be found here. The abstract from the paper is the following:Transformer based models have achieved state-of-the-art results in many Natural LanguageProcessing tasks. However, their memory footprint, inference latency, and power consumption are prohibitive for efficient inference at the edge, and even at the data center. In this work, we propose I-BERt, a novel quantization scheme for Transformerbased models that quantizes the entire inference with integer-only arithmetic. Based on lightweight integer- only approximation methods for nonlinear operations, e.g., GELU, Softmax, and Layer Normalization, I-berT performs an end-to-end integer- Only BERT inference without any floating point calculation. We evaluate our approach on GLUE downstream tasks usingRoBERTa-Base/Large. We show that for both cases, I -BERT achieves similar (and slightly higher) accuracy as compared to the full-precision baseline.