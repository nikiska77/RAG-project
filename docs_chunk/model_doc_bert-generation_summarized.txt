The BertGeneration model is a BERT model that can be leveraged for sequence-to-sequence tasks. The model can be used in combination with the [EncoderDecoderModel] to leverage two pretrained BERT checkpoints for fine-tuning. The models result in new state-of-the-art results on Machine Translation,Text Summarization, Sentence Splitting, and Sentence Fusion. The original code can be found here. The code for the Bert2Bert model was contributed by patrickvonplaten. The full code is available here. It can also be downloaded from the GitHub repository here. For more information, see the paper Leveraging Pre-trained Checkpoints for Sequence Generation by Sascha Rothe, Shashi Narayan, Aliaksei Severyn and Shashank Narayan. The abstract from the paper is the following:Unsupervised pretraining of large neural models has recently revolutionized Natural Language Processing. By warm-starting from the publicly released checkpoints, NLP practitioners have pushed the state- of-the.art on multiple benchmarks. This paper aims to demonstrate the efficacy of pre-trained checkpoints for Sequence. Generation. The paper is open-source and can be downloaded by clicking here.