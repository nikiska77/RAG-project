The LUKE model was proposed in LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda and Yuji Matsumoto. It is based on RoBERTa and adds entity embeddings as well as an entity-aware self-att attention mechanism. It helps improve performance on tasks involving reasoning about entities such as named entity recognition,extractive and cloze-style question answering, entity typing, and relation classification. This model was contributed by ikuyamada and nielsr. The original code can be found here. You can obtain those using the LUKE website or the GitHub page. The abstract from the paper is the following:.Entity representations are useful in natural language tasks involving entities. In this paper, we propose new.pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and. entities in a given text as independent tokens, and outputs contextualized. representations of them. Our model is trained using a new pretraining task based on. the masked language model of BERT. The task involves predicting randomly masked words and entity in a large entity-annotated corpus retrieved from Wikipedia.