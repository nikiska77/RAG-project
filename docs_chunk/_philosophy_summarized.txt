Transformers is an opinionated library built for machine learning researchers and educators. The library was designed with two strong goals in mind:Be as easy and fast to use as possible.Provide state-of-the-art models with performances as close as possible to the original models. Use regular Python, PyTorch, TensorFlow, Keras modules and inherit from the base classes of the library to reuse functionalities like model loading and saving. If you'd like to learn more about our coding philosophy for models, check out our Repeat Yourself blog post. We strongly limited the number of user-facing abstractions to learn. There are almost no abstractions, in fact, there are just three standard classes required to use each model: configuration, models, and a preprocessing class. We provide at least one example for each architecture which reproduces a result provided by the official authors of said architecture. The code is usually as close to the code as possible, but we encourage you to experiment with your own code. We hope you'll find the library useful for your own research and development. We're looking forward to hearing from you! We hope to hear from you soon! We'll post back with more information about the library and its features. Back to the page you came from.