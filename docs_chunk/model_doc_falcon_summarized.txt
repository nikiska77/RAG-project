Falcon is a class of causal decoder-only models built by TII. The largest Falcon checkpoints have been trained on >=1T tokens of text. Falcon models are (as of 2023) some of the largest and most powerful open-source language models. They consistently rank highly in the OpenLLM leaderboard. Falcon is now fullysupported in the Transformers library. You can convert custom code checkpoints to full Transformers checkpoints using the convert_custom_code_checkpoint.py script located in the Falcon model directory of theTransformers library. If your model hasn't been uploaded to the Hub, we recommend making a backup before attempting the conversion, just in case. If you fine-tuned a model from a custom code checkpoint, werecommend converting your checkpoint to the new in-library format, as this should give significant improvements to stability and performance. For more information, visit the Falcon Hub. The Falcon Hub can be downloaded from the Hugging Face Hub or the Transformers Hub from the following URL: HuggingFaceHub.org/Falcon. The Transformers Hub is available under the Apache 2.0 license and is available in the following languages: English, German, French, Spanish, Italian, Portuguese, Spanish and Russian. It is based on the Haskell programming language.