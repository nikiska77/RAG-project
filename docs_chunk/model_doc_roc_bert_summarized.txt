The model was proposed in RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou. It's a pretrained Chinese language model that is robust under various forms of adversarial attacks. The model takes as input multimodal information including thesemantic, phonetic and visual features. We show all these features are important to the model robustness since the attack can be performed in all the three forms. Across 5 Chinese NLU tasks, ROCBERT outperforms strong baselines under three blackbox adversarial algorithms without sacrificing the performance on clean testset. It also performs the best. in the toxic content detection task under human-made attacks. It is pretrained with the contrastive learning objective which maximizes the label consistency. under different synthesized adversarial examples. It was developed by weiweishi and the paper was published in the journal Computational Linguistic Theory and Research (CLR) (http://www.clr.org/content/lpr/2013/01/08/07/lr-lpr.html).