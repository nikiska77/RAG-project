EfficientFormer proposes a dimension-consistent pure transformer that can be run on mobile devices for dense prediction tasks like image classification. The fastest model, EfficientFormer-L1, achieves 79.2% top-1 accuracy on ImageNet-1K with only 1.6 ms inference latency on iPhone 12 (compiled with CoreML), which runs as fast as MobileNetV2×1.4 (1.6ms, 74.7%Top-1) The largest model,    Efficient former-L7, obtains 83.3% accuracy with only 7.0 ms latency on the iPhone 12. The paper was written by Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren. It was published in the journal Computer Vision and Pattern Recognition (CVPR) (http://www.cvpr.org/content/vcr/2008/01/07/efficient-former-vision-and-pattern-recognition-vcr.html#vcr_vcr), and it was published by the journal Computational Vision and Prediction (CVP) ( http:// www.cva.org/.