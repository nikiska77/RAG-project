The Wav2Vec2 model was proposed in wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. It is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. The model was trained using connectionist temporal classification (CTC) so the model output has to be decoded using [Wav2 Vec2CTCTokenizer]. There is a list of official Hugging Face and community resources to help you get started with the model. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. For more information on how to use the model, please visit the Wav1Vec1 project page or the wav1vec2 project page. for more information about how to leverage the model for emotion classification, please go to the WAV1VEC2 Project page. For the full version of this article, please see the official Wav3Vec3 project page, which includes a guide to using the model with n-grams.