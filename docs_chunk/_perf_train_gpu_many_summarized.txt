If training a model on a single GPU is too slow or if the model's weights do not fit in a singleGPU's memory, transitioning to a multi-GPU setup may be a viable option. Multiple techniques can be employed to achieve parallelism, such as data parallelism and tensor parallelism. There isn't a one-size-fits-all solution, and the optimal settings depend on the specific hardware configuration you are using. The main concepts discussed in this guide are likely applicable across frameworks, but here we focus on PyTorch-based implementations. For step-by-step tutorials on distributed training, please refer to the ðŸ¤— Accelerate documentation. For models hosted on the Hub, use our Model Memory Calculator, which gives you accurate calculations within a few percent margin. For more information on training models on a large infrastructure, please visit the #TrainYourModelHub page or the #TrainingHub page for more information about how to train your model on large infrastructure. The #TrainHerHub page is also available on the GitHub page for training models with large amounts of vRAM on aÂ largeÂ infrastructure. TheÂ #TrainHer Hub page for largeÂ infrastructuresÂ is also available at theÂ https://www.crt.org/crt/training/training-models-on-a-large-infrastructure-withÂ v RAM.