The ELECTRA model was proposed in the paper ELECTRA: Pre-training Text Encoders as Discriminators Rather ThanGenerators. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model. This new pretraining task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. The gains are particularly strong for small models. We train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute. This model was contributed by lysandre. The original code can be found here. The only change is the separation of the embedding size and the hidden hidden. The model is the pretraining approach, therefore there is nearly no changes done to the underlying model: BERT. The changes are the only change.