Transformers provides a class optimized for training. It's easier to start training without manually writing your own training loop. The [Trainer] provides API for hyperparameter search. This doc shows how to enable it in example. It supports four backends currently: optuna, sigopt, raytune and wandb. You can pass direction inhyperparameter_search and define your own search space. You should install them before using them as the hyper parameter search backend. For example, install optuna/sigopt/wandb/ray[tune] and pip install optunasigopt.ip install wandb/raysgopt.pip install optuno/raytune.ipinstall wandb/.pipinstall optuna.ip installed optuna_hp_space.ip installation optuna-hp-search.ip.py installoptuna-hpo.py optuna:hpo-search-search, pip:hpa-search (optuna:hp-hp-search), pip-parameter.py sigopt:pip:pparameter (sig Optuna), pip:p Parameter (pparam Optuna, pparam Sigopt), pparam sigopt (pParam Sigopt, pParam sigopt)