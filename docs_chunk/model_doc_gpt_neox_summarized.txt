GPT-NeoX-20B is a 20 billion parameter autoregressive language model trained on the Pile. The model is open-source and available to the public through a permissive license. It is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. The generate() method can be used to generate text using GPT Neo model. The training and evaluation code, as well as the model weights, can be found at https://github.com/EleutherAI/gpt-neox-20b. It was trained with fp16 and has a different tokenizer from the one used in G PT-J-6B and GPT -Neo. The new tokenizer allocates additional tokens to whitespace characters, making the model more suitable for certain tasks like code generation. It can be downloaded from GitHub at: http://www.gptneox.org/gPT-neoX20B/GPT Neo is a language model with a 20B-parameter autore progressive language model. It has been developed by Sid Black, Stella Biderman and Eric Hallahan, with the support of CoreWeave.