The Nezha model was proposed in NEZHA: Neural Contextualized Representation for Chinese Language Understanding by Junqiu Wei et al. The current version of NeZHA is based on BERT with a collection of proven improvements, which include Functional Relative Positional Encoding as an effective positional encoding scheme. The experimental results show that NEzHA achieves the state-of-the-art performances when finetuned on several representative Chinese tasks, including entity recognition (People's Daily NER), sentence matching (LCQMC), Chinese sentiment classification (ChnSenti) and natural language inference (XNLI). This model was contributed by sijunhe. The original code can be found here. The abstract from the paper is the following: The pre-trained language models have achieved great successes in various natural language understanding (NLU) tasks. It is hoped that this article will shed light on some of the techniques used in the training of the models. The model has been updated to make it easier for people to use the model in a variety of situations. It has also been improved to make the training more efficient. The full report can be downloaded from: http://www.cnn.com/2013/01/29/science/science-and-technology/nezha-pre-trained-language-models.html#storylink=cpy.