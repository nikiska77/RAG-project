This model is in maintenance mode only, so we won't accept any new PRs changing its code. This model was deprecated due to security issues linked to pickle.load. We recommend switching to more recent models for improved security. If you run into any issues running this model, please reinstall the last version that supported this model: v4.35.0.1. The Transfo-XL model was proposed in Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, RuslanSalakhutdinov. It's a causal (uni-directional) transformer with relative positioning (sinuso√Ødal) embeddings which canreuse previously computed hidden-states to attend to longer context (memory) This model also uses adaptive softmaxinputs and outputs (tied) The abstract from the paper is the following:Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the settings of language modeling. We propose a novel neural architecture that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a Novel positional encoding scheme.