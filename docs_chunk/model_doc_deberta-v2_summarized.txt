The DeBERTa model is based on Google's BERT and Facebook's RoBERTa. It builds on RoberTa with disentangled attention and enhanced mask decoder training with half of the data used in BERT. It is the second version of the DeberTa model. It includes the 1.5B model used for the SuperGLUE single-model submission and achieving 89.9, versus human baseline 89.8. The DeBER Ta code andpre-trained models will be made publicly available at https://github.com/Microsoft/DeBERTa-v2. The abstract from the paper is the following: Recent progress in pre-trained neural. language models has significantly improved the performance of many natural.language processing (NLP) tasks. In this paper we propose a new model architecture DeBER.Ta (Decoding-enhanced BERT with Disentangled Attention) that improves the Bert and RoBER Ta models using two novel techniques. We show that these two techniques significantly improve the efficiencyof model pretraining and performance of downstream tasks. Compared to RoBER ta-Large, a DeBER ta model trained on half of. the training data performs consistently better on a wide range of NLP tasks. It achieves improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs 90.7%) and RACE by +3.6% (83.2%. vs 86.8%).