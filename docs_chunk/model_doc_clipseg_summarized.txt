CLIPSeg adds a minimal decoder on top of a frozen CLIP model for zero- and one-shot image segmentation. A prompt can be either a text or an image (provided to the model as conditional_pixel_values). One can also provide customconditional embeddings. The system generates a binary segmentation map for an image based on a free-text prompt or on an additional image expressing the query. This approach enables us to create a unified model(trained once) for three common segmentation tasks, which are referred expression segmentation, zero-shot segmentation andOne-shot Segmentation. We find our system to adapt well to generalized queries involving affordances or properties. The model was contributed by nielsr and the original code can be found here. The original paper on the subject was written by Timo LÃ¼ddecke and Alexander Ecker and published in Image Se segmentation Using Text and Image Prompts. The full paper can be downloaded from the paper's website at: http://www.researchers.com/image-segmentation-using-text-and-image-prompts-in-a-machine-learning-toolkit/article/view.php?title=Image-Segmentation-Using-Text-And-Image- Prompts-In-A Machine-Learning-Toolkit.