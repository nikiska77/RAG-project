The Autoformer model augments the Transformer as a deep decomposition architecture. It can progressively decompose the trend and seasonal components during the forecasting process. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. The original code can be found here. A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started is also available at HuggingFace.com. The Autoformer blog-post in Hugging face blog: Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer) is available on the Huggingface.com website. The blog post was written by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long, and elisim and kashif. The abstract from the paper is the following:Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long- term energy consumption planning.