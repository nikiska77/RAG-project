The BORT model was proposed in Optimal Subarchitecture for BERT by Adrian de Wynter and Daniel J. Perry. It is an optimal subset of architectural parameters for the BERT, which theauthors refer to as "Bort" It is demonstrably smaller, having an effective (that is, not counting the embedding layer) size of 5.5% the original BERT-large architecture. Bort is also able to be pretrained in 288 GPU hours, which is 1.2% of the time required to pretrain the highest-performing BERT parametric architectural variant, RoBERTa-large(Liu et al., 2019) It is also 7.9x faster on a CPU, as well as being better performing than other compressed variants of the compressed BERT architecture. It obtains performance improvements of between 0.3% and 31%,absolute, on multiple public natural language understanding (NLU) benchmarks. The model is in maintenance mode only, we do not accept any new PRs changing its code. If you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.0You can do so by running the following command: pip install -U transformers==4.29.0-4.31.