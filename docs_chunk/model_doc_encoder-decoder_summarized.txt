The [EncoderDecoderModel] can be used toinitialize a sequence-to-sequence model with any pretrained autoencoding model as the encoder. The effectiveness of initializing models with pretrained checkpoints for sequence generation tasks was shown in Leveraging Pre-trained Checkpoints for Sequence Generation Tasks by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. An application of this architecture could be to leverage two pretrained [BertModel] as the. encoder and decoder for a summarization model as was. shown in: Text Summarization with Pretrained Encoders by Yang Liu and Mirella Lapata. Note that any. pretrained auto-encoding model, e.g. BERT, can serve as theEncoder and both pretrained. auto- Encoder and auto-Encoding models, e.,g. GPT2, as well as the pretrained decoder part of sequence- to-sequence. models, such as BART, can also be used as the decoder. In the following example, we show how to do this using the default [B BertModel] configuration for the encoders and the default. [BERTForCausalLM] Configuration for the decoders.