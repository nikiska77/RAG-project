Accelerated Linear Algebra, dubbed XLA, is a compiler for accelerating the runtime of TensorFlow Models. It can be triggered with the jit_compile argument in any graph-creating function such as tf.function. XLA is not limited to these methods - it can also be used to accelerate any arbitrary TF function. Several Tensor Flow methods have been rewritten to be XLA-compatible, including text generation for models such as GPT2, T5 and OPT. We noticed a speed-up of ~100x for text generation models inside ðŸ¤— Transformers. This document will explain how you can use XLA for these models to get the maximum amount of performance. Weâ€™ll also provide links to additional resources if youâ€™re interested to learn more about the benchmarks and our design philosophy behind the XLA integration. For more information on XLA and how to use it in your project, visit XLA in the tensorflow GitHub repository or go to the Xla in the Tensorflow README section of the README. The XLA documentation can be downloaded from the GitHub site here: http://www.tensorflow.org/xla/XLA is a domain-specific compiler for linear algebra that can accelerate Tensor flow models.