UniSpeech-SAT is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. The model can be fine-tuned using connectionist temporal classification (CTC) so the model output has to be decoded using [Wav2Vec2CTCTokenizer]. The model performs especially well on speaker verification, speaker identification, and speaker diarization tasks. This model was contributed by patrickvonplaten. The Authors' code can be found here. For more information on the UniSpeechSat model, see the official website here. It can be downloaded from the GitHub site here: http://www.justgiving.com/sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu. The model was proposed in UniSpeach-Sat: Universal Speech Representation Learning with Speaker Aware pre-Training. The abstract from the paper is the following: "Self-supervised learning (SSL) is a long-standing goal for speech processing, since it utilizes large-scale unlabeled data and avoids extensive human labeling. In this paper, we aim to improve the existing SSL framework for speaker representation learning"