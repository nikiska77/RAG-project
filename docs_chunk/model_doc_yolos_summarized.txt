The YOLOS model was proposed in You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu. It uses the plain Vision Transformer (ViT) for object detection, inspired by DETR. YOLos pre-trained on the mid-sized ImageNet-1k dataset only can already achieve quite competitive performance on the challenging COCO benchmark. The original code can be found here. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. All example notebooks illustrating inference + fine-tuning [YolosForObjectDetection] on a custom dataset can befound here. Use [YOLosImageProcessor] for preparing images (and optional targets) for the model. The model was contributed by nielsr and the original code is available on GitHub. It can be downloaded from the GitHub repository here. It is currently being developed by the Yolos team.