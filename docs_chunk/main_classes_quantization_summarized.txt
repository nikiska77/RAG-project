AWQ method has been introduced in the AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration paper. With AWQ you can run models in 4-bit precision, while preserving its original quality. Currently the integration with Transformers is only available for models that have been quantized using autoawq library and llm-awq. Note that using AWQ requires to have access to a NVIDIA GPU. CPU inference is not supported yet. We advise users to look at different existing tools in the ecosystem to quantize their models with AWQ algorithm. You can find in the sections below some example snippets and notebook. examples of how to use AWQ with Transformers. We now support inference with any AWQ model, meaning anyone can load and use AW Q weights that are pushed on the Hub or saved locally. We hope you will find this information helpful. We are happy to answer any questions you may have about AWQ and Transformers in the comments below. Back to the page you came from. Click here for more information on AWQ, Transformers, and other tools to help you with your work with the tool. Back To the page where you camefrom. Click the link for more details about the tool, and to see the example snippets.