Luxembourg-based LUKE model is extended with entity embeddings. This helps improve performance on various downstream tasks. It can be used to train a multilingual language model with 24 languages. The model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks. This model was contributed by ryo0634. The original code can be found here. You can directly plug in the weights of the mLUKE model into a LUKE. model, like so: model = LukeModel.from_pretrained("studio-ousia/mluke-base") model.tokenizer = MLukeTokenizer.fromÂ pretrained(LUKE) model. tokenizer.weights = weights.weights. weights. weights (LUKE, LUKE). weights ( LUKE, LUKES, LUCES). tokensizer. weights = weights (Lukes, LECS, LUNES, MUNIS, MULTIPLAYER, MELISSA, MESSEYS, MALESTS, MOLESTS). tokensize. tokensizer = tokens. tokensize (LUKES) tokenizer: tokensize(LUCES, LMUNIS), tokensizer: tokenizer (MUNIS) tokensizer (LUNS, MOUSE, MALKIN, MACHINES)