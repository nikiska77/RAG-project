The model was proposed in a paper by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou. The abstract from the paper is the following:Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks. In this paper, we present LayoutLMv2 by pre-training text, layout and image in a multi-modal framework, where new modelarchitectures and pre- training tasks are leveraged. Experiment results show that LayoutLMV2 outperforms strong baselines andachieves new cross-modality interaction is better learned. It also integrates a spatial-aware self-attention Attention Mechanism into the Transformer architecture, so that the model can fully understand the relative positional positionalrelationship among different text blocks. The paper was published in the open-source journal, Cascades, on November 14, 2013. It is available for download from the Mac OS X App Store and from the Google Play Store for iOS and the Android App Store for Android. For confidential support, call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or click here for details.