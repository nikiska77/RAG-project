The Whisper model was proposed in Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks. When compared to humans, the models approach their accuracy and robustness. Inference is currently only implemented for short-form i.e. audio is pre-segmented into <=30s segments. Long-form (including timestamps) will be implemented in a future release. Here is a step-by-step guide to transcribing an audio sample using a pre-trained WhisperModel. The original code can be found here. The WhisperProcessor can be used to prepare audio for the model, and decode the predicted ID's back into text. To convert the model and the processor, we recommend using the following:. python src/transformers/models/whisper/convert_openai_to_hf.py --checkpoint_path "" --pytorch_dump_ folder_path "Arthur/Whisper-3" --convertÂ preprocessor True.