Code Llama is a family of large language models for code based on Llama 2. It provides state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. It is released under a permissive license that allows for both research and commercial use. Check out all Code Llama model checkpoints here and the officially released ones in the codellama org. The original code of the authors can be found here. The model was contributed by ArthurZucker. It was trained using bfloat16, but the original inference uses b float16. The Llama2 family models, on which Code Llamas is based, were trained using a different type of float, bfloat15. It has been suggested that the model could be used in a number of other languages, such as C, C++, C#, Python, and Haskell. It can also be used to create a Python-based version of the Python programming language. The code for the model was written by Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin.