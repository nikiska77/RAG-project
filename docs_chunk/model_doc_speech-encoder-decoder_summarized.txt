The [SpeechEncoderDecoderModel] can be used to.initialize a speech-to-text model with any pretrained speech autoencoding model as the encoder (e.g. Wav2Vec2, Hubert) and any. pretrained autoregressive models as the decoder. The effectiveness of initializing speech-sequence- to-text-sequence models with pretrained checkpoints for speechrecognition and speech translation has been shown in. Large-Scale Self- and Semi-Supervised Learning for Speech.Translation by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, and.Alexis Conneau. We show how to do this using the default [Wav2 Vec2Model] configuration for the. encoder and default [BertForCausalLM] for the decoder in the following example. It is possible to use a Speech2Text2 model for inference in a similar way to the above example. We call this the SpeechEncoder Decoder Model (SCD) and it can be found in the README for Speech2 Text2. It can also be used as a base for other speech models, such as GPT2, BART, and BERT.