The ViTMatte model was proposed in Boosting Image Matting with Pretrained Plain Vision Transformers by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang. It leverages plain Vision Transformers for the task of image matting, which is the process of accurately estimating the foreground object in images and videos. The model uses a hybrid attention mechanism combined with a convolution neck to help ViTs achieve an excellent performance-computation trade-off in matting tasks. It inherits many superior properties from ViT to matting including various pretraining strategies, concise architecture design, and flexible inference strategies. It achieves state-of-the-art performance and outperforms prior matting works by a large margin. A demo notebook regarding inference with [VitMatteForImageMatting], including background replacement, can be found here. A list of official Hugging Face and community resources (indicated by ðŸŒŽ) resources to help you get started with Vi TMatte can be seen here. The original code for the model is available here. It expects both the image and trimap (concatenated) as input. Use [ViTMatmeImageProcessor] for this purpose. It can be downloaded from here.