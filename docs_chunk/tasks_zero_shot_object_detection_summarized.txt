OWL-ViT leverages multi-modal representations to perform open-vocabulary detection. It combines CLIP with lightweight object classification and localization heads. The model can detect objects based on textual descriptions without prior training on labeled datasets. In this guide, you will learn how to use OWL- ViT to detect objects in images based on text prompts and batch object detection for image-guided object detection. We'll use an image of astronaut Eileen Collins that is a part of the NASA Great Images dataset. The simplest way to try out the model is to use it in a [pipeline]. Instantiate a pipeline for zero-shot object detection from a checkpoint on the Hugging Face Hub: authenticate-checkpoint = "google/owlvit-base-patch32" test-pip = pipeline(model=checkpoint, task="zero-shot-object-detection") test-ip=pip.pip install -q transformers. transformers transformerstransformers.transformers (pip: "pip"): transformers(pip://://://www.google.com/search/pip/viz/pix/vz/vx/px/vk/vq/pz/pk/pv/vp/vm/vl/vf/vc/vn/vw/vt/vh/vj/vg/vv/mv/s/vlt.vz.vf.vm.vv.vk.vc.vx"): vn.vh.vw.vg.vj.vp.vip.ip = vv.ip.vms.vq.vn.pv.m. vk.pix.vb.v.s.vr.vl.vnc.vpn.vkn.vtn.vhn.vbn.vni.vna.vne.vmn.vnz.vyn.vwn.vnn.vgn.vinc.vnt.vlon.vno.vsm.vnone.