CodeGen is an autoregressive language model for program synthesis trained sequentially on The Pile, BigQuery, and BigPython. The model was proposed in A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autore progressive language modeling. In addition, our model CodeGen (with up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the HumanEval benchmark. We make the training library JaxFormer including checkpoints available as open source contribution: this https URL. The original code can be found here. The format is: Salesforce/codegen-{size}-{data}. where sizeofsize: 350M, 2B, 6B, 16B, 14B, 15B, 20B, 25B, 30B, 35B, 40B, 45B, 50B, 55B, 60B, 65B, 70B, 75B, 80B, 85B, 90B, 95B, 100, 90, 95, 100.