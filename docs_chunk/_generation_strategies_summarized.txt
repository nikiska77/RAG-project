Text generation is essential to many NLP tasks, such as open-ended text generation, summarization, translation, and more. Some of the models that can generate text include GPT2, XLNet, OpenAI GPT, CTRL, TransformerXL, XLM, Bart, T5, GIT, Whisper. The process of selecting output tokens to generate text is known as decoding. Modifying a decoding strategy does not change the values of any trainable parameters. However, it can have a noticeable impact on the quality of the generated output. The default configuration is also used when no custom configuration has been saved with the model. A decoding strategy for a model is defined in its generation configuration. This guide describes common decoding strategies and their main parameters. saving and sharing custom generation configurations with your fine-tuned model on Hub is a good way to get the most out of your NLP models.summarize:                 Text generation strategies                . The decoding strategy that a model uses for text generation can be customized to suit your needs and the type of text you are trying to generate. For more information, see the Hub's NLP guide: Text generation strategies for NLP and speech-to-text. The Hub's text generation strategy guide can be downloaded from the Hub website.