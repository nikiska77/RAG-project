RoFormer is a BERT-like autoencoding model with rotary position embeddings. It has shown improved performance on classification tasks with long texts. The model was contributed by junnyu. The original code can be found here. The undergoing experiment for English benchmark will soon be updated. The theoretical analysis along with some preliminary experiment results on Chinese data will be released soon. The paper was written by Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen and Yunfeng Liu. It was published in the open-source journal Cascades (http://www.cascades.com/blog/2013/01/26/ro-former-with-rotary-position-embedding-roPE.html#storylink=cpy) The paper is also available on the arXiv.com site, where you can download a copy of the code for free. It is available in English, Chinese, Japanese, and Korean. The abstract from the paper is the following:Position encoding in transformer architecture provides supervision for dependency modeling between elements at                different positions in the sequence. We investigate various methods to encode positional information. The proposed RoPE encodes absolute positional information with rotation matrix and incorporates explicit relativeposition dependency in self-attention formulation. As a result, the enhancedtransformer with Rotary Position Embedding, or RoFormer, achieves superior performance in tasks withLong texts.