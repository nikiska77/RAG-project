The Transformers library is very opinionated with respect to model code. Each model should fully be implemented in a single file without relying on other models. We have added a mechanism that checks whether a copy of the code of a given model stays consistent with the original. This way, when there is a bug fix, we can see all other impacted models and choose to trickle down the modification or break the copy. If a file is a full copy of another file, you should register it in the constant FULL_COPIES of utils/check_copies.py. This mechanism relies on comments of the form # Copied from xxx. The xxx should contain the whole path to the class of function which is being copied below. For instance here you can see how RobertaPreTrainedModel._init_weights is copied from the same method in Bert pre-trainedModel. The code is copied with all instances of foo being replaced by bar. You can add several patterns separated by a comma. For example here CamemberForMaskedLM is a direct copy of RobertaFor MaskedLM with two replacements: Roberta to Camembert and ROBERTA to CAMEMBER. The copy is exactly the same except for names: for instance in RobertaAttention, we use RobertaSelfAttention insted of Bert selfAttention.