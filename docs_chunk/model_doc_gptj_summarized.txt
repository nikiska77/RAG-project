The GPT-J model was released in the kingoflolz/mesh-transformer-jax repository by Ben Wang and Aran Komatsuzaki. It is a G PT-2-like language model trained on the Pile dataset. It would take at least 48GB of RAM to just load the model. To reduce the RAM usage there are a few options. The torch_dtype argument can be used toinitial the model in half-precision on a CUDA device only. There is also a fp16 branch which stores the fP16 weights, which could be used for training/fine-tuning. The model should fit on 16GB GPU for inference. For training/ fine-tuned it would take much more GPU RAM, even with mixed precision as gradient updates are in fp32. Another option is to use the original codebase to train the model on TPU and then convert the model to Transformers format for inference, which would again require some moreGPU RAM. Although the embedding matrix has a size of 50400, only 50257 entries are used by the Gpt-2 tokenizer. These extra extra tokens are added to the tokenizer to make it more efficient. For more information on how to use Transformers, visit the Transformers website.