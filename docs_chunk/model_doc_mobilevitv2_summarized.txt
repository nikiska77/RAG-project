The MobileViTV2 model was proposed in Separable Self-attention for Mobile Vision Transformers by Sachin Mehta and Mohammad Rastegari. The improved model is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection. It achieves a top-1 accuracy of 75.6% on the ImageNet dataset, outperforming MobileViT by about 1% while running 3.2Ã— faster on a mobile device. It does not work on sequence data but on batches of images. Unlike ViT, there are no embeddings. The backbone model outputs a feature map. The available image classification checkpoints are pre-trained on ImageNet-1k (also referred to as ImageNet 1k) The model is more like a CNN than a Transformer model. It uses element-wise operations for computing self-att attention, making it a good choice for resource-constrained devices. One can use [MobileViTImageProcessor] to prepare images for the model. Note that if you do your own preprocessing, the pretrained checkpoints expect images to be in BGR pixel order (not RGB). The model was contributed by shehan97. The original code can be found here.