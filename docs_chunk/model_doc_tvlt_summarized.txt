Textless Vision-Language Transformer (TVLT) is a model that uses raw visual and audio inputs for vision-and-language representation learning. It can perform various audiovisual and vision-language tasks like retrieval, question answering, etc. TVLT attains performance comparable to its text-based counterpart on various multimodal tasks, such as visual question answering. It has 28x faster inference speed and only 1/3 of the parameters. It is trained by reconstructing masked patches of continuous video frames and audio spectrograms (masked autoencoding) and contrastive modeling to align video and audio. One can use [TvltProcessor] to prepare data for the model. It wraps an image processor (for the image/video modality) and an audio feature extractor ( for the audiomodality) into one. The original code can be found here. This model was contributed by Zineng Tang. The abstract from the paper is the following: In this work, we present the Textless vision- language Transformer. The model takes both pixel_values and audio_values as input. The authors resize and crop the input images/videos to 224 and limit the length of audio spectrogram to 2048. To make batching of videos and audios possible, the authors use a pixel_mask that indicates which pixels are real/padding.