The OpenAI GPT-2 model was proposed in Language Models are Unsupervised Multitask Learners. It's a causal (unidirectional)transformer pretrained using language modeling on a very large corpus of ~40 GB of text data. The model can take the past_key_values (for PyTorch) or past (for TF) as input, which is the previously computed key/value attention pairs. It is available in five sizes: small, medium, large, xl and a distilled version of the small checkpoint: distilgpt-2. The original code was contributed by thomwolf. The current version of this article is based on the OpenAI version of GPT2, which can be downloaded from the Open AI website here. It has been updated to include the latest version of OpenAI’s GPT model, as well as the latest versions of the Python bindings for Pytorch and TF. For more information on how to use GPT 2, see the Openai GPT1 and GPT3 versions of this blog post. For the full version of these instructions, go to the GitHub site here: http://www.openai.com/gpt2/ and click on the ‘GPT2’ button.