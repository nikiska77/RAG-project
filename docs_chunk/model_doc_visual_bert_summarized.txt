The model was proposed in VisualBERT: A Simple and Performant Baseline for Vision and Language by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang. It consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an image with self-attention. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERt outperforms or rivals with state-of-the-art models while being significantlysimpler. It can be used for visual question answering, multiple choice, region-to-phrase correspondence tasks, and other tasks. The model was contributed by gchhablani. The original code can be found here. It will be available in the research package, but the states can be loaded directly into the detector provided. For the VCR task, the authors use a fine-tuned detector for generating visual embeddings, for all the checkpoints. If you are not working on these downstream tasks, it is recommended that you use the pretrained checkpoints. The detector and its weights as a part of the package are not available.