The DPT model was proposed in Vision Transformers for Dense Prediction by Ren√© Ranftl, Alexey Bochkovskiy, Vladlen Koltun. It is a model that leverages the Vision Transformer (ViT) as a backbone for dense prediction tasks like semantic segmentation and depth estimation. The model is compatible with the [AutoBackbone] class. This allows to use the DPT framework with various computer vision backbones available in the library, such as [VitDet backbone] or [Dinov2Backbone]. One can create it as follows:thon from transformers import Dinov2Config, DPTForDepthEstimation, D PTFor DepthEstimation. The original code can be found here. The abstract from the paper is the following:We introduce dense vision transformers, an architecture that Leverages vision transformer in place of convolutional networks. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network.