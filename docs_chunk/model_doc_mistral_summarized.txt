Mistral-7B-v0.1 is a decoder-based LM with the following architectural choices: Sliding Window Attention - Trained with 8k context length and fixed cache size. Byte-fallback BPE tokenizer - ensures that characters are never mapped to out of vocabulary tokens. We also provide an instruction fine-tuned model: Mistral- 7B-Instruct-v 0.1 which can be used for chat-based inference. Both models are released under the Apache 2.0 license. For more details please read our release blog post. The model can be downloaded and used via the HuggingFace Hub. The full release of the Mistral AI software is available on the GitHub repository: http://www.mistralai.com/release/mistral/7B/Mistral AI's first Large Language Model (LLM) is available for download from the GitHub site. It is available in English, German, French, Spanish, Italian, Portuguese, Spanish and Arabic. It has a theoretical attention span of 128K tokens with a theoretical lifetime of 1,000,000 words. It can also be used as a training model for other languages such as English and French. It was designed to be able to be used in conjunction with other languages like German and Spanish.