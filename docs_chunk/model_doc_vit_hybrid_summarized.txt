Hybrid Vision Transformer (ViT Hybrid) model was proposed in An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining very good results compared to familiar convolutional architectures. ViT hybrid is a slight variant of the plain Vision Trans transformer, leveraging a Convolutional backbone (specifically, BiT) whose features are used as initial "tokens" for the Transformer. The model was contributed by nielsr. The original code (written in JAX) can be found here. A list of official Hugging Face and community resources to help you get started with ViT Hybrid is available here. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing existing resource. The Hugging face community is open-source, so please share your ideas and feedback with us on Twitter @HuggingFace or on our Facebook page. We'd like to hear from you! Send us an email at jennifer.smith@mailonline.co.uk or send us a message via the form below.