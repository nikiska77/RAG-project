IDEFICS is an open-access vision and language model based on Flamingo. The model accepts arbitrary sequences of image and text inputs and generates coherent text as output. It can answer questions about images, describe visual content, and create stories grounded in multiple images. IDEFICS comes in two variants - 80 billion parameters and 9 billion parameters, both of which are available on the Hub. For each variant, you can also find fine-tuned instructed versions of the model adapted for conversational use cases. To run the following examples with a non-quantized version of theModel, you will need at least 20GB of GPU memory. Let us know if you have any questions about this guide or how to use it in the comments below or post a comment on our Facebook page about our guide to the IdEFICS Hub. We would like to hear from you about how you use the model and how you can use it to solve image-text tasks. Please send us a message via the comments section at the bottom of the page. We are happy to answer any questions you may have about the model or its use in our guide. We hope this guide will help you with your image captioning and captioning tasks. For more information, visit the IDEFics Hub.