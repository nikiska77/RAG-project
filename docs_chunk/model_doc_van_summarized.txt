The VAN model was proposed in Visual Attention Network by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu. While extremely simple, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation. The figure below illustrates the architecture of a Visual Aattention Layer.This model was contributed by Francesco. The original code can be found here. Code is available at this https URL.Tips:VAN does not have an embedding layer, thus the hidden_states will have a length equal to the number of stages. The model is in maintenance mode only, we don't accept any new PRs changing its code. If you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.0 and run the following command: pip install -U transformers==4.29.0 -u transformers=4.31.0-v1.30-v2.30/v3.2.2-v4.3.