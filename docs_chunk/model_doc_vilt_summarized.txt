The ViLT model was proposed in ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Wonjae Kim, Bokyung Son, Ildoo Kim. ViLT incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for VLP training. The model takes both pixel_values and input_ids as input. One can use [ViltProcessor] to prepare data for the model. The original code can be found here. The quickest way to get started with ViLT is by checking the example notebooks (which showcase both inference and fine-tuning on custom data) The authors resize the shorter edge of input images to 384 and limit the longer edge to under 640 while preserving the aspect ratio. To make batching of images possible, the authors use a pixel_mask that indicates  the size of the image. The authors use an image processor (for the image modality) and a tokenizer ( for the languagemodality) into one. This processor wraps a image processor and a tokensizer into one to make it easier to train the model with different sizes of images. It is called the Vilt Processor and can be downloaded from the GitHub site.