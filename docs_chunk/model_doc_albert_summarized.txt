The ALBERT model was proposed in ALBERt: A Lite BERT for Self-supervised Learning of Language Representations by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. It uses repeating layers which results in a small memory footprint, however the computational cost remains high. The model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. It is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than the left. It's more logical to have H >> E instead of H >> V for the embedding matrix since it's V x E (V being the vocab size) The model jax version was contributed by lysandre. The original code can be found here. This model jx version was contributions by kamalkraj. The full model can be downloaded from the GitHub repository here. It can also be downloaded as a zip file from the repository below.