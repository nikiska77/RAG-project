SwitchTransformers model was proposed in Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity by William Fedus, Barret Zoph, Noam Shazeer. The model uses a sparse T5 encoder-decoder architecture, where the MLP are replaced by a Mixture of Experts (MoE) A routing mechanism associates each token to one of the expert, where each expert is a dense MLP. While switch transformers have a lot more weights than their equivalent dense models, the sparsity allows better scaling and better finetuning performance at scale. During a forward pass, only a fraction of the weights are used. The routing mechanism allows the model to select relevant weights on the fly which increases the model capacity without increasing the number of operations. The released weights are pretrained on English Masked Language Modeling task, and released weights were used to train the model in English. The original code can be found here. The paper was published in the open-source journal, Cascades, on November 14, 2013. It is available on the arXiv.com/2013/0108.037. It was published by the Open Data Institute (ODI) on November 13, 2013, on the request of the Institute of Electrical and Electronics Engineers.