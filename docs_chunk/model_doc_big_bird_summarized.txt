The BigBird model was proposed in Big Bird: Transformers for Longer Sequences. BigBird is a sparse-attention transformer which extends Transformer based models, such as BERT to much longer sequences. In addition to sparseattention, BigBird also applies global attention as well as random attention to the input sequence. Theoretically, it has been shown that applying sparse, global, and random attention approximates full attention. As a consequence of the capability to handle longer context, Big Bird drastically improves performance on various NLP tasks such as question answering and summarization. The proposed sparse attention can handle sequences of length up to 20x of what was previously possible using similar hardware. We also propose novel applications to genomics data. For an in-detail explanation on how BigBird's attention works, see this blog. The original code can be found here. For tips on how to use BigBird, see the BigBird blog. For more information on the Big Bird project, please visit the Big bird website. The Big Bird blog is open-source and can be accessed via the following URL: http://www.bigbird.org/blog/2014/01/07/big-bird-transformer-model-for-longer-sequences.