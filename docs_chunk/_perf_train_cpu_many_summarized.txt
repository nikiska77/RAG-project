This guide focuses on PyTorch-based DDP enabling distributed CPU training efficiently. OneCCL (collective communications library) is a library for efficient distributed deep learning training implementing collectives like allreduce, allgather, alltoall. For more information on oneCCL, please refer to the oneccL documentation and oneCCl specification. The files are available for the following Python versions: Python 3.6, 3.7, Python3.8 and 3.9, and the latest version of Python, 1.12.0. The guide is based on Intel's OneClerk, an open-source library for training on multiple CPUs. It is available in the following versions: 1.13.0, 2.2.0 and 2.3.0 (and the latest release of Python). The guide includes the following modules: oneccl_bindings_for_pytorch (torch_ccl before version 1.1.12)  implements PyTorCh C10D ProcessGroup API and can be dynamically loaded as external ProcessGroup and only works on Linux platform now. It also includes the oneclerk-bindings-for-pyTorch module, which can be downloaded from the GitHub repository.