The AltCLIP model was proposed by Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, Ledell Wu. It is a neural network trained on a variety of image-text and text-text pairs. By switching CLIP'stext encoder with a pretrained multilingual text encoder XLM-R, we could obtain very close performances with CLIP on almost all tasks. We set new state-of-the-art Performances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and COCO-CN. It can be used for image- text similarity and for zero-shot image classification. The model is very similar to the CLIP, except that we use bidirectional attention instead of casual attention and we take the [CLS] token in XLS-R to represent text embedding. It's a multi-modal vision and language model, and it was developed by Chen, Liu, Zhang, Wu, Ye, Yang, and Wu. The paper is titled Altering the Language Encoder in CLIP for Extended Language Capabilities. It was published in the journal Computer Vision and Pattern Recognition (CPR)