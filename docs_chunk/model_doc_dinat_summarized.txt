DiNAT was proposed in Dilated Neighborhood Attention Transformer by Ali Hassani and Humphrey Shi. DiNAT is a new hierarchical vision transformer built upon both the sliding-window Neighborhood Attention (NA) and Swin Transformer's Shifted Window Self Attention. DiNA is faster and ahead of its Swin counterpart by 1.5% box AP in COCO object detection, 1.3% mask AP in ADE20K semantic segmentation, and 1.1% mIoU in CocO instance segmentation. It also matches the state of the art specialized semantic segmentations models on ADE 20K (58.2 mIOU), and ranks second on Cityscapes (84.5 mIiU) (no extra data). * DiNA has different dilation values for different areas of Neighborhood Attention with different dilated values. For more information on DiNA, visit the DiNAT website. The full paper is available at: http://www.researchers.com/diNAT/DiNAT.html. It is also available as a free download on the MIT Open Source Software Project website, and can be downloaded for free on the Apple iOS and Android platforms, as well as the Google Play Store.