BridgeTower was proposed in BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan. The paper has been accepted to the AAAI'23 conference. The goal of this model is to build abridge between each uni-modal encoder and the cross-moda encoder to enable comprehensive and detailed interaction at each layer. The model achieves remarkable performance on various downstream tasks with almost negligible additional performance and computational costs. It was pre-trained with only 4M images, BRIDGETOWER achieves state-of-the-art performance onVarious downstream vision-language tasks. It achieves an accuracy of 78.73% on the VQAv2 test-std set. It surpasses models that are pre- trained on orders- of-magnitude larger datasets. It enables effective bottom-up cross- modal alignment and fusion between visual and textual representations of different semantic levels of pre- Training data and pre-training data. It has almost negligible extra parameters and computational cost. It can be scaled up to a much larger dataset, such as that used for the QAv2 tests.