Perplexity (PPL) is one of the most common metrics for evaluating language models. PPL is defined as the exponentiated average negative log-likelihood of a sequence. It can be thought of as an evaluation of the model's ability to predict uniformly among the set of specified tokens in a corpus. The largest version of GPT-2, for example, has a fixed length of 1024 tokens, so we can't calculate PPL directly when the number of tokens is greater than the size of the context. For more information about perplexity and its relationship to Bits Per Character (BPC) and data compression, check out this blog post on The Gradient. Back to the page you came from. Click here to read the next part of the series on how to calculate perplexity with fixed-length models. The next part will look at the perplexity of language models with masked language models like BERT. The final part will focus on how PPL can be calculated with approximate language models such as GPT and GPT+2. Click the next page for the next section on the discussion about how to Calculate PPL with approximate models. It will also look at how perplexity can be computed with language models that are masked. The last section will look into how to compute perplexity for classical language models (sometimes called causal language models)