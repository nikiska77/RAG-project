The LXMERT model was proposed in a paper by Hao Tan & Mohit Bansal. It is a series of bidirectional transformer encoders pretrained using a combination of masked language modeling, visual-language text alignment, ROI-feature regression, maskedvisual-attribute modeling, masked visual-object modeling, and visual-question answering objectives. After fine-tuning from our pretrained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA) We also show the generalizability of our model by adapting it to a challenging visual-reasoning task, NLVR, and improve the previous best result by 22% absolute (54% to 76%).. We demonstrate detailed ablation studies to prove that both our novel model components and pretraining strategies significantly contribute to our strong results. We also present several visualizations for visualattention visualizations. For more information, visit LXMert.org. The abstract from the paper is the following:Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision- and-language connections.