CLIP is a multi-modal vision and language model. It can be used for image-text similarity. The model was proposed in Learning Transferable Visual Models From Natural Language Supervision by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. It is a neural network trained on a variety of (image, text) pairs. After pre-training, natural language is used to referencelearned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero- shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL. The original code can be found here and the code for the CLIP model was contributed by valhalla. Back to Mail Online home. Back To the page you came from. The post was originally published on November 14, 2013. Back into the page.