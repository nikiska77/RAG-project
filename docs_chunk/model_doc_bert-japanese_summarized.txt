The BERT models are trained on Japanese text. There are models with two different tokenization methods.Tokenize with MeCab and WordPiece. This requires some extra dependencies. To use MecabTokenizer, you should pip install transformers ["ja"] (or pip install -e.["ja"] if you install from source) to install dependencies. The implementation is the same as BERT, but with Japanese text instead of English. This model was contributed by cl-tohoku.summarize:                 BertJapanese                              ‘’’, ‘””, “’,. ’”,. ”’. ”. ’.”.’%.’;. ‘.  ’: ‘;’.;. “. ”: ’;”; . ‚’+.’.: ’,.’!’%: ’,’.,’,'’',’%,’%’}.’%;. •’.-’#’+’$’()’&’$.’(’/’)’).’.$’('#”+”%”%,”%.”&”/”# % ;‚”!”",’",”@’|’?’}’;"’";’();’%-’true’);’ol’ (’fugashi’), ‘fugashim’): ‘#fugASHIM’ });.