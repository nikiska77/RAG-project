OWLv2 was proposed in Scaling Open-Vocabulary Object Detection by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2 scales up OWL-ViT using self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. This results in large gains over the previous state-of-the-art for zero-shot object detection. With an L/14 architecture, OWl-ST improves AP on LVIS rare classes, for which the model has seen no human box annotations, from 31.2% to 44.6% (43% relative improvement)OWL-ST unlocks Web-scale training for open-world localization, similar to what has been seen for image classification and language modelling. The original code can be found here. The abstract from the paper is the following:Open-vocabulary object detection has benefited greatly from pretrained vision-language models, but is still limited by the amount of available detection training data. Here, we scale up detection data with self- Training. Major challenges in scaling self- training are the choice of label space, pseudo-annotation filtering, and training efficiency. We present the OW Lv2 model and OWL St self-Training recipe, which address these challenges.