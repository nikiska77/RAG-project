DINOv2 is an upgrade of DINO, a self-supervised method applied on Vision Transformers. This method enables all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self- supervised methods, can produce such features if trained on enough curated data from diverse sources. The model was contributed by nielsr. The original code can be found here. It can also be traced using torch. The abstract from the paper is the following:The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all- Purpose visual features. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all- purpose features, OpenCLIP (Ilharco et al, 2021) on most of the benchmarks at image and pixel levels. In. terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data.