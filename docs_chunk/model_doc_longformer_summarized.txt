The Longformer model was presented in Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a localwindowed attention with a task motivated global attention. Since the Longformer is based on RoBERTa, it doesn't have token_type_ids. You don't need to indicate which segment a token belongs to. Just separate your segments with the separation tokenizer.sep_token (or                   ). A transformer model replacing the attention matrices by sparse matrices to go faster. The authors' code can be found here. The model was contributed by beltagy and Peters, and the code was written by Cohan and Beltagy. It can be downloaded from the GitHub repository here: http://www.jupiter.com/project/Longformer. Back to the page you came from. Back To the pageyou came from:http:// www.josh-smith.org/blog/2013/01/26/longformer-the-long-Document-transformer.html. Back into the page. Back onto the page that you came From. The page that came from is the page to which you came.